<!DOCTYPE html>
<html>
  <head>
    <title>Table Annotation using Deep Learning</title>
    <link
      rel="stylesheet"
      href="https://webdatacommons.org/style.css"
      type="text/css"
      media="screen"
    />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <style>
      .tar {
        text-align: right;
      }
      .rtable {
        float: right;
        padding-left: 10px;
      }
      .smalltable,
      .smalltable TD,
      .smalltable TH {
        font-size: 9pt;
      }
      .tab {
        overflow: hidden;
        border: 1px solid #ccc;
        background-color: #eaf3fa;
        clear: both;
        padding-left: 25px;
        width: 650px;
      }
      .tab button {
        background-color: inherit;
        float: left;
        border: none;
        outline: none;
        cursor: pointer;
        padding: 15px 60px;
        transition: 0.3s;
      }
      .tab button:hover {
        background-color: #ddd;
      }
      .tab button.active {
        background-color: #ccc;
      }
      .tabcontent {
        display: none;
        padding: 6px 12px;
        border-top: none;
        animation: fadeEffect 1s;
        width: 500px;
      }
      .table-wrapper {
        position: relative;
      }
      .table-scroll {
        height: 240px;
        overflow: auto;
        margin-top: -10px;
      }
      .show {
        display: block;
      }
      .no-show {
        display: none;
      }
      caption {
        caption-side: top;
        font-style: italic;
      }
      td[scope="mergedcol"] {
        text-align: center;
      }
      tr.bordered {
        border-bottom: 1px solid #000;
      }
      hr {
        width: 50%;
        margin: 20px 0;
        /* This leaves 10px margin on left and right. If only right margin is needed try margin-right: 10px; */
      }
      .tg {
        border-collapse: collapse;
        border-color: #ccc;
        border-spacing: 0;
      }
      .tg td {
        background-color: #fff;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg th {
        background-color: #f0f0f0;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        font-weight: normal;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg tr th {
        align-items: center;
      }

      .tg {
        vertical-align: top;
      }
      .tg .tg-lboi {
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-9wq8 {
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-ixdq {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: center;
        top: -1px;
        vertical-align: middle;
        will-change: transform;
      }
      .tg .tg-mkpc {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: left;
        top: -1px;
        vertical-align: middle;
        will-change: transform;
      }
      .tg .tg-d459 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-kyy7 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-h2b0 {
        background-color: #fff;
        border-color: inherit;
        color: #333;
        text-align: center;
        vertical-align: middle;
      }
      .tg-sort-header::-moz-selection {
        background: 0 0;
      }
      .tg-sort-header::selection {
        background: 0 0;
      }
      .tg-sort-header {
        cursor: pointer;
      }
      .tg-sort-header:after {
        content: "";
        float: right;
        margin-top: 7px;
        border-width: 0 5px 5px;
        border-style: solid;
        border-color: #404040 transparent;
        visibility: hidden;
      }
      .tg-sort-header:hover:after {
        visibility: visible;
      }
      .tg-sort-asc:after,
      .tg-sort-asc:hover:after,
      .tg-sort-desc:after {
        visibility: visible;
        opacity: 0.4;
      }
      .tg-sort-desc:after {
        border-bottom: none;
        border-width: 5px 5px 0;
      }
      @media screen and (max-width: 767px) {
        .tg {
          width: auto !important;
        }
        .tg col {
          width: auto !important;
        }
        .tg-wrap {
          overflow-x: auto;
          -webkit-overflow-scrolling: touch;
        }
      }
      .tg {
        border-collapse: collapse;
        border-color: #ccc;
        border-spacing: 0;
      }
      h3 {
        padding-bottom: 1rem;
      }
      .tg td {
        background-color: #fff;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg th {
        background-color: #f0f0f0;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        font-weight: normal;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg .tg-lboi {
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-9wq8 {
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-baqh {
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-zyik {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: center;
        top: -1px;
        vertical-align: top;
        will-change: transform;
      }
      .tg .tg-ixdq {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: center;
        top: -1px;
        vertical-align: middle;
        will-change: transform;
      }
      .tg .tg-yy5h {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-o939 {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-ufyq {
        background-color: #f0f0f0;
        font-weight: bold;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-asv9 {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-d459 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-dzk6 {
        background-color: #f9f9f9;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-kyy7 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg-sort-header::-moz-selection {
        background: 0 0;
      }
      .tg-sort-header::selection {
        background: 0 0;
      }
      .tg-sort-header {
        cursor: pointer;
      }
      .tg-sort-header:after {
        content: "";
        float: right;
        margin-top: 7px;
        border-width: 0 5px 5px;
        border-style: solid;
        border-color: #404040 transparent;
        visibility: hidden;
      }
      .tg-sort-header:hover:after {
        visibility: visible;
      }
      .tg-sort-asc:after,
      .tg-sort-asc:hover:after,
      .tg-sort-desc:after {
        visibility: visible;
        opacity: 0.4;
      }
      .tg-sort-desc:after {
        border-bottom: none;
        border-width: 5px 5px 0;
      }
      @media screen and (max-width: 767px) {
        .tg {
          width: auto !important;
        }
        .tg col {
          width: auto !important;
        }
        .tg-wrap {
          overflow-x: auto;
          -webkit-overflow-scrolling: touch;
        }
      }
      h2 {
        padding-bottom: 1rem;
      }
      .tg {
        border-collapse: collapse;
        border-color: #ccc;
        border-spacing: 0;
        display: inline-block;
        margin-right: 50px;
      }
      .tg td {
        background-color: #fff;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg th {
        background-color: #f0f0f0;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        font-weight: normal;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg .tg-lboi {
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-pl4i {
        background-color: #f0f0f0;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-j1i3 {
        border-color: inherit;
        position: -webkit-sticky;
        position: sticky;
        text-align: left;
        top: -1px;
        vertical-align: top;
        will-change: transform;
      }
      .tg .tg-9wq8 {
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-ixdq {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: center;
        top: -1px;
        vertical-align: middle;
        will-change: transform;
      }
      .tg .tg-yy5h {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-o939 {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-45e1 {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-nrix {
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-d459 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-kyy7 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-57iy {
        background-color: #f9f9f9;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-zkss {
        background-color: #fff;
        border-color: inherit;
        color: #333;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-p91v {
        background-color: #ff0;
        border-color: inherit;
        color: #333;
        font-weight: bold;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-c3ow {
        border-color: inherit;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-syo5 {
        background-color: #f0f0f0;
        border-color: inherit;
        color: #333;
        font-weight: bold;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-h2b0 {
        background-color: #fff;
        border-color: inherit;
        color: #333;
        text-align: center;
        vertical-align: middle;
      }
      .tg-sort-header::-moz-selection {
        background: 0 0;
      }
      .tg-sort-header::selection {
        background: 0 0;
      }
      .tg-sort-header {
        cursor: pointer;
      }
      .tg-sort-header:after {
        content: "";
        float: right;
        margin-top: 7px;
        border-width: 0 5px 5px;
        border-style: solid;
        border-color: #404040 transparent;
        visibility: hidden;
      }
      .tg-sort-header:hover:after {
        visibility: visible;
      }
      .tg-sort-asc:after,
      .tg-sort-asc:hover:after,
      .tg-sort-desc:after {
        visibility: visible;
        opacity: 0.4;
      }
      .tg-sort-desc:after {
        border-bottom: none;
        border-width: 5px 5px 0;
      }
      @media screen and (max-width: 767px) {
        .tg {
          width: auto !important;
        }
        .tg col {
          width: auto !important;
        }
        .tg-wrap {
          overflow-x: auto;
          -webkit-overflow-scrolling: touch;
        }
      }
      @keyframes fadeEffect {
        from {
          opacity: 0;
        }
        to {
          opacity: 1;
        }
      }
      .left,
      .right {
        display: inline-block;
      }
      .center {
        display: block;
        margin-left: auto;
        margin-right: auto;
        width: 30%;
      }
      figure figcaption {
        text-align: center;
      }
    </style>
    <script
      type="text/javascript"
      src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"
    ></script>
    <script type="text/javascript" src="../../jquery.toc.min.js"></script>
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(["_setAccount", "UA-30248817-1"]);
      _gaq.push(["_trackPageview"]);

      (function () {
        var ga = document.createElement("script");
        ga.type = "text/javascript";
        ga.async = true;
        ga.src =
          ("https:" == document.location.protocol
            ? "https://ssl"
            : "http://www") + ".google-analytics.com/ga.js";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(ga, s);
      })();
    </script>
    <script charset="utf-8">
      var TGSort =
        window.TGSort ||
        (function (n) {
          "use strict";
          function r(n) {
            return n ? n.length : 0;
          }
          function t(n, t, e, o = 0) {
            for (e = r(n); o < e; ++o) t(n[o], o);
          }
          function e(n) {
            return n.split("").reverse().join("");
          }
          function o(n) {
            var e = n[0];
            return (
              t(n, function (n) {
                for (; !n.startsWith(e); ) e = e.substring(0, r(e) - 1);
              }),
              r(e)
            );
          }
          function u(n, r, e = []) {
            return (
              t(n, function (n) {
                r(n) && e.push(n);
              }),
              e
            );
          }
          var a = parseFloat;
          function i(n, r) {
            return function (t) {
              var e = "";
              return (
                t.replace(n, function (n, t, o) {
                  return (e = t.replace(r, "") + "." + (o || "").substring(1));
                }),
                a(e)
              );
            };
          }
          var s = i(/^(?:\s*)([+-]?(?:\d+)(?:,\d{3})*)(\.\d*)?$/g, /,/g),
            c = i(/^(?:\s*)([+-]?(?:\d+)(?:\.\d{3})*)(,\d*)?$/g, /\./g);
          function f(n) {
            var t = a(n);
            return !isNaN(t) && r("" + t) + 1 >= r(n) ? t : NaN;
          }
          function d(n) {
            var e = [],
              o = n;
            return (
              t([f, s, c], function (u) {
                var a = [],
                  i = [];
                t(n, function (n, r) {
                  (r = u(n)), a.push(r), r || i.push(n);
                }),
                  r(i) < r(o) && ((o = i), (e = a));
              }),
              r(
                u(o, function (n) {
                  return n == o[0];
                })
              ) == r(o)
                ? e
                : []
            );
          }
          function v(n) {
            if ("TABLE" == n.nodeName) {
              for (
                var a = (function (r) {
                    var e,
                      o,
                      u = [],
                      a = [];
                    return (
                      (function n(r, e) {
                        e(r),
                          t(r.childNodes, function (r) {
                            n(r, e);
                          });
                      })(n, function (n) {
                        "TR" == (o = n.nodeName)
                          ? ((e = []), u.push(e), a.push(n))
                          : ("TD" != o && "TH" != o) || e.push(n);
                      }),
                      [u, a]
                    );
                  })(),
                  i = a[0],
                  s = a[1],
                  c = r(i),
                  f = c > 1 && r(i[0]) < r(i[1]) ? 1 : 0,
                  v = f + 1,
                  p = i[f],
                  h = r(p),
                  l = [],
                  g = [],
                  N = [],
                  m = v;
                m < c;
                ++m
              ) {
                for (var T = 0; T < h; ++T) {
                  r(g) < h && g.push([]);
                  var C = i[m][T],
                    L = C.textContent || C.innerText || "";
                  g[T].push(L.trim());
                }
                N.push(m - v);
              }
              t(p, function (n, t) {
                l[t] = 0;
                var a = n.classList;
                a.add("tg-sort-header"),
                  n.addEventListener("click", function () {
                    var n = l[t];
                    !(function () {
                      for (var n = 0; n < h; ++n) {
                        var r = p[n].classList;
                        r.remove("tg-sort-asc"),
                          r.remove("tg-sort-desc"),
                          (l[n] = 0);
                      }
                    })(),
                      (n = 1 == n ? -1 : +!n) &&
                        a.add(n > 0 ? "tg-sort-asc" : "tg-sort-desc"),
                      (l[t] = n);
                    var i,
                      f = g[t],
                      m = function (r, t) {
                        return n * f[r].localeCompare(f[t]) || n * (r - t);
                      },
                      T = (function (n) {
                        var t = d(n);
                        if (!r(t)) {
                          var u = o(n),
                            a = o(n.map(e));
                          t = d(
                            n.map(function (n) {
                              return n.substring(u, r(n) - a);
                            })
                          );
                        }
                        return t;
                      })(f);
                    (r(T) ||
                      r((T = r(u((i = f.map(Date.parse)), isNaN)) ? [] : i))) &&
                      (m = function (r, t) {
                        var e = T[r],
                          o = T[t],
                          u = isNaN(e),
                          a = isNaN(o);
                        return u && a
                          ? 0
                          : u
                          ? -n
                          : a
                          ? n
                          : e > o
                          ? n
                          : e < o
                          ? -n
                          : n * (r - t);
                      });
                    var C,
                      L = N.slice();
                    L.sort(m);
                    for (var E = v; E < c; ++E)
                      (C = s[E].parentNode).removeChild(s[E]);
                    for (E = v; E < c; ++E) C.appendChild(s[v + L[E - v]]);
                  });
              });
            }
          }
          n.addEventListener("DOMContentLoaded", function () {
            for (var t = n.getElementsByClassName("tg"), e = 0; e < r(t); ++e)
              try {
                v(t[e]);
              } catch (n) {}
          });
        })(document);
    </script>
    <!-- MathJax: cdn to display latex equations -->
    <script
      type="text/javascript"
      async=""
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"
    ></script>
  </head>
  <body>
    <div id="logo" style="text-align: right; background-color: white">
      &nbsp;&nbsp;<a href="http://dws.informatik.uni-mannheim.de"
        ><img src="images/ma-logo.gif" alt="University of Mannheim - Logo"
      /></a>
    </div>
    <div id="header">
      <h1 style="font-size: 250%">Table Annotation using Deep Learning</h1>
    </div>
    <div id="authors">
      <a>Chun-Yi Chen</a><br />
      <a>I-Chen Hsieh</a><br />
      <a>Munir Abobaker</a><br />
      <a>Reng Chiz Der</a><br />
      <a>Keti Korini (Supervisor)</a><br />
      <a>Christian Bizer (Supervisor)</a><br />
    </div>

    <div id="content">
      <p>
        This webpage engages with the experiements and results of a six month
        student team project on the topic of "Table Annotation using Deep
        Learning" at the School of Business Informatics and Mathematics of the
        University of Mannheim under the supervision of Keti Korini and
        Christian Bizer.
      </p>
      <p>
        We have conducted a set of experiments around the use of transformer
        language models for table annotation, based on
        <a href="https://webdatacommons.org/structureddata/sotab/"
          >WDC Schema.org Table Annotation Benchmark (SOTAB)</a
        >[<a href="#toc9">1</a>]. There are two multi-class classification
        tasks:
        <a href="https://paperswithcode.com/task/column-type-annotation"
          >Column Type Annotation (CTA)</a
        >
        and
        <a href="https://paperswithcode.com/task/columns-property-annotation"
          >Columns Property Annotation (CPA)</a
        >. CTA involves predicting the column type, while CPA involves
        identifying semantic column relationships between different column pairs
        in a table. All of our experiments and code are available in our
        <a
          href="https://github.com/wbsg-uni-mannheim-students/table-annotation-using-deep-learning"
          >GitHub repository.</a
        >
      </p>

      <p>
        This work is organized as follows. Firstly, in Chapter 1, we will
        introduce and motivate the task of table annotation. Furthermore, in
        Chapter 2, we will provide an overview of the table annotation tasks.
        Subsequently, in Chapter 3, we will present the related work including,
        Transformer model. Moving on to Chapter 4, we will describe the models,
        preprocessing steps, and serialization approaches and hyperparameter
        tuning. Additionally, in Chapter 5, we will describe the experiment
        setup, results and error analysis. Finally, in Chapters 6, we will
        discuss the results and conclude the work.
      </p>
      <h2>Contents</h2>
      <ul>
        <li class="toc-h2 toc-active">
          <a href="#toc1"> 1. Introduction</a>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc2"> 2. Theoretical Background</a>
          <ul>
            <li>
              <a href="#toc2.1">2.1 CTA</a>
            </li>
            <li>
              <a href="#toc2.2">2.2 CPA</a>
            </li>
          </ul>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc3"> 3. Related Work</a>
          <ul>
            <li>
              <a href="#toc3.1">3.1 Transformer</a>
            </li>
            <li>
              <a href="#toc3.2">3.2 TURL</a>
            </li>
            <li>
              <a href="#toc3.3">3.3 TUTA</a>
            </li>
            <li>
              <a href="#toc3.4">3.4 DODUO</a>
            </li>
          </ul>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc4"> 4. Model </a>
          <ul>
            <li>
              <a href="#toc4.1">4.1 Preprocessing and Augmentation</a>
            </li>
            <li>
              <a href="#toc4.2">4.2 Serialization</a>
            </li>
            <li>
              <a href="#toc4.3">4.3 Further Models</a>
            </li>
          </ul>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc5"> 5. Experimental Approach and Evaluation </a>
          <ul>
            <li>
              <a href="#toc5.1">5.1 Dataset</a>
            </li>
            <li>
              <a href="#toc5.2">5.2 Experimental Settings</a>
            </li>
            <li>
              <a href="#toc5.3">5.3 Experiments for SOTAB benchmark</a>
            </li>
            <li>
              <a href="#toc5.4">5.4 Experiments for WikiTables dataset</a>
            </li>
          </ul>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc6"> 6. Experimental Results and Error Analysis </a>
          <ul>
            <li>
              <a href="#toc6.1">6.1 Main Results</a>
            </li>
            <li>
              <a href="#toc6.2">6.2 Hyperparameter Tuning</a>
            </li>
            <li>
              <a href="#toc6.3">6.3 Error Analysis</a>
            </li>
          </ul>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc6"> 7. Discussion and Conclusion </a>
          <ul>
            <li>
              <a href="#toc6.1">6.1 Discussion</a>
            </li>
            <li>
              <a href="#toc6.2">6.2 Conclusion</a>
            </li>
            <li>
              <a href="#toc6.3">6.3 Acknowledgments</a>
            </li>
          </ul>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc8"> 7. References </a>
        </li>
      </ul>
      <span id="toc1"></span>
      <h2>1. Introduction</h2>
      <p>
        <a href="https://paperswithcode.com/task/column-type-annotation"
          >Table annotation</a
        >
        is the task of annotating a table with terms/concepts from knowledge
        graph, database schema or vocabulary. This process is crucial for data
        management including data quality control, data discovery, and schema
        matching process [<a href="#toc17">7</a>], especially since the web
        contains millions of tables available on websites, public data portals,
        and encyclopedia such as Wikipedia. However, to utilize these tables, we
        need to understand their structure and schema, which can be challenging
        due to their heterogeneous or unknown headers and content. The objective
        of this project is to address the aforementioned challenges by
        proficiently utilizing diverse deep learning methods for both Column
        Type Annotation and Columns Property Annotation tasks.
      </p>

      <span id="toc2"></span>
      <h2>2. Theoretical Background</h2>
      <p>
        This Chapter provides an overview of the theoretical underpinnings,
        frameworks as well as specific information relevant to follow our work.
        For this reason, we start by introducing the main tasks we are trying to
        solve.
      </p>

      <span id="toc2.1"></span>
      <h3>2.1 Column Type Annotation (CTA)</h3>
      <p>
        Column Type Annotation (CTA) is a critical task in data integration and
        knowledge discovery [<a href="#toc14">4</a>] that involves assigning
        labels to each column in a table based on the types of entities it
        contains such as
        <b
          >"hotel/name", "streetAddress","addressLocality", "Country", and
          "currency" </b
        >[<a href="#toc9">1</a>] as shown in <i>Figure 1</i>. While data types
        provide basic information about the type of data stored in a column,
        entity types capture domain-specific semantics that convey more meaning.
        For instance, labeling a column with <b>"hotel/name"</b> rather than
        just <b>"String" </b> provides valuable information about the specific
        type of data stored in that column, which is crucial for downstream
        tasks such as data analysis and retrieval [<a href="#toc24">16</a>].
      </p>

      <p>
        An example of CTA: An e-commerce website may have a table of products
        with columns such as "product name", "description", "price", "category",
        and "brand". By annotating the columns with entity types such as
        <b>"product name"</b>, <b>"brand"</b>, and <b>"category"</b>, a machine
        learning model can better understand the semantic meaning of the data,
        and accurately categorize products into appropriate categories,
        recommend related products, and provide a more personalized shopping
        experience to the user.
      </p>

      <p>
        CTA can be approached as either
        <a href="https://paperswithcode.com/task/column-type-annotation/">
          a multi-class or a multi-label classification</a
        >
        problem. In multi-class, each column is annotated with only one label
        representing its type, while in multi-label, each column is annotated
        with multiple labels.
      </p>

      <span id="toc2.2"></span>
      <h3>2.2 Columns Property Annotation (CPA)</h3>
      <p>
        CPA is a web data integration task and it plays a crucial role in
        providing understanding in the semanntics of a table for better table
        understanding [<a href="#toc17">7</a>]. It aims to identify semantic
        relationships between different columns in a table and can infer
        relationships that might not be immediately obvious. As illustrated in
        <i>Figure 1</i>, CPA can help to infer that the <b>"address" </b>column
        refers to the hotel's location.
      </p>

      <p>
        One common way to tackle CPA is to view it as a
        <a href="https://paperswithcode.com/task/columns-property-annotation/">
          multi-class classification</a
        >
        task, and it is also known as column relation annotation or relation
        extraction in various works.
      </p>

      <span id="toc2.1.Fig"></span>
      <figure>
        <img
          src="images/table_CTA_CPA.jpg"
          alt="Table Annotation Example"
          class="center"
        />
        <figcaption class="center">
          <b><a id="Fig1"></a>Figure 1:</b> Example of table annotation. The CTA
          labels are placed on the top of the columns, while the CPA labels are
          placed between the main column (leftmost column) and another column
          [<a href="#toc9">1</a>]
        </figcaption>
      </figure>

      <span id="toc3"></span>
      <h2>3. Related Work</h2>
      <span id="toc3.1"></span>

      <p>
        Chapter 3 provides an overview of relevant literature on table
        annotation tasks and a Transformer-based language model, which is
        currently the state-of-the-art approach for these tasks. We also
        describe the datasets used to evaluate our approaches.
      </p>
      <h3>3.1 Transformer</h3>
      <p>
        Transformers are a type of neural network architecture that has become
        widely used in natural language processing and other machine learning
        applications. They were first introduced in 2017 by Vaswani et al [<a
          href="#toc15"
          >5</a
        >] and have since become one of the most popular deep-learning models
        [<a href="#toc24">17</a>]. Transformers are based on the concept of
        self-attention, which allows the model to focus on different parts of
        the input sequence at each step.
      </p>

      <p>
        The key innovation of transformers is self-attention, a mechanism that
        allows the model to selectively focus on different parts of the input
        sequence. Self-attention works by computing a weighted sum of the input
        sequence at each position, where the weights are determined by a
        similarity function between the current position and all other
        positions. The output of the self-attention layer is a sequence of
        weighted sums, which can then be fed into subsequent layers of the
        network. The attention function is formalized as:
      </p>
      <p>
        <span class="math display"
          >\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span
        >
      </p>
      <p>
        where Q, K, and V are the query, key, and value vectors, respectively.
      </p>
      <p>
        A Transformer-based language model is ideal for the tasks due to its
        language understanding capabilities and Transformer's attention
        mechanism that captures contextualized column representations.
      </p>
      <span id="toc3.2"></span>
      <h3>3.2 TURL</h3>
      <p>
        The paper [<a href="#toc12">4</a>] provides a cornerstone for table
        representation through the introduction of a framework for table
        understanding called TURL. TURL achieves table understanding by learning
        deep contextualized representations of relational tables utilizing
        unsupervised pre-training on around 570K relational tables from
        Wikipedia. The pre-training data is constructed based on the WikiTable
        corpus [<a href="#toc26">18</a>], a corpus containing around 1.65M
        tables extracted from Wikipedia pages.
      </p>
      <p>
        The universal architecture of TURL facilitates its application in
        various downstream tasks, requiring only minimal task-specific
        modifications. For example, the TURL framework can be used to infer
        valuable information about tables such as column types and the relationships between columns (CTA and CPA). The framework, as shown in
        <a id="Fig2">Figure 2</a>,is composed of three main components: an
        embedding layer that first converts input tables into embeddings, an
        N-stacked aware transformer layer that attends over these embeddings to
        capture useful information, and a projection layer for pre-training
        objectives. In addition to a Masked Language Model (MLM) used by
        standard BERT language model [<a href="#toc22">14</a>] that can learn
        representations for tokens in table metadata, TURL proposes a Masked
        Entity Recovery (MER) objective to learn entity cell representations.
        Lastly, to perform the pre-training, TURL leverages a pre-trained
        TinyBERT model [<a href="#toc27">19</a>] and pre-trained the model for
        80 epochs.
      </p>

      <span id="toc3.2.Fig"></span>
      <figure>
        <img
          src="images/TURL.PNG"
          alt="TURL"
          class="center"
          width="100%"
          height="auto"
        />
        <figcaption class="center">
          <b><a id="Fig2"></a>Figure 2:</b> Overview of TURL framework [<a
            href="#toc12"
            >4</a
          >]
        </figcaption>
      </figure>

      <span id="toc3.3"></span>
      <h3>3.3 TUTA</h3>
      <p>
        TUTA, as introduced in [<a href="#toc16">8</a>], represents the first
        transformer-based model for the CTA and CPA tasks. Notably, TUTA
        demonstrates the importance of incorporating structure-aware mechanisms
        for table representation tasks. It captures the spatial and hierarchical
        information of tables through a tree-based attention and position
        mechanism. TUTA inspired us to incorporate similar preprocessing and
        structural-based approaches in our work.
      </p>

      <span id="toc3.4"></span>
      <h3>3.4 DODUO</h3>
      <p>
        DODUO, as described in [<a href="#toc15">7</a>], is the curent
        state-of-the-art approach for the CTA and CPA tasks. This approach
        utilizes a Transformer-based language model and takes the entire table
        as input to predict column types and relationships between columns. What
        sets DODUO apart to its predecessors is its ability to annotate table
        columns using only the information contained in the table itself,
        without the need for external knowledge or context.
      </p>

      <p>
        As shown in <a id="Fig3">Figure 3</a>, DODUO utilizes a pre-trained
        Transformer-based language model, particularly a BERT language model [<a
          href="#toc22"
          >14</a
        >] and adopts multi-task learning into the model to "transfer" shared
        knowledge between CTA and CPA tasks. DODUO introduces table-wise
        serialization to incorporate table context into prediction.
        Particularly, the serialization method is: for a table of N columns of m
        rows,
        <span class="math display"
          >\[serialize(T)::= [CLS]\;v^1_1\;...[CLS]\;
          v^1_n...v^n_m\;[SEP]\]</span
        >
        where a [CLS] token is appended to the beginning of each column, which
        its corresponding embeddings represent learned column representations
        for the column. The output layer on top of a column embedding, i.e. the
        [CLS] token, is used for the CTA task. Column embeddings of a column
        pair is used for the CPA task instead.
      </p>

      <span id="toc3.4.Fig"></span>
      <figure>
        <img
          src="images/DODUO.PNG"
          alt="DODUO"
          class="center"
          width="100%"
          height="auto"
        />
        <figcaption class="center">
          <b><a id="Fig3"></a>Figure 3:</b> Overview of DODUO's model
          architecture [<a href="#toc15">7</a>]
        </figcaption>
      </figure>

      <span id="toc3.5"></span>
      <h3>3.5 TaBERT</h3>
      <p>
        TaBERT [<a href="#toc28">20</a>] is a pretrained language model built on
        top of BERT that jointly learns representations for natural language
        (NL) sentences and (semi-)structured tables. The contributions of TaBERT
        include linearization of the structure of tables to be compatible with
        Transformer-based BERT model, content snapshots to encode subset of
        relevant table content to cope with large tables, and vertical attention
        mechanism that share information across table rows. Lastly, TaBERT is
        pretrained on a corpus of 26 million tables and English paragraphs to
        capture the association between tabular data and related NL text.
      </p>
      <p>
        The overall architecture for TaBERT is shown in
        <a id="Fig4">Figure 4</a>. Firstly, a content snapshot of a table is
        created based on the input NL utterance. Row-wise encodings are produces
        for utterance tokens and cells. Lastly, all row-wise encodings are
        aligned and processed by vertical self-attention layers to generate
        utterance and column representations. These representations are then
        used for downstream tasks.
      </p>

      <span id="toc3.5.Fig"></span>
      <figure>
        <img
          src="images/TaBERT.JPG"
          alt="TaBERT framework"
          class="center"
          width="100%"
          height="auto"
        />
        <figcaption class="center">
          <b><a id="Fig4"></a>Figure 4:</b> Overview of TaBERT [<a href="#toc28"
            >20</a
          >]
        </figcaption>
      </figure>

      <span id="toc3.6"></span>
      <h3>3.6 Dataset</h3>
      <p>
        We evaluated our solutions primarily on the
        <a href="https://webdatacommons.org/structureddata/sotab/"
          >WDC Schema.org Table Annotation Benchmark (SOTAB)</a
        >[<a href="#toc9">1</a>] benchmark dataset. To ensure the
        generalizability of our approach, we test our best serialization
        approaches on the
        <a href="https://github.com/sunlab-osu/TURL/"
          >WikiTables dataset by TURL</a
        >[<a href="#toc12">4</a>]. The SOTAB benchmark is a collection of tables
        extracted from the
        <a href="http://webdatacommons.org/structureddata/schemaorgtables/"
          >Schema.org Table Corpus</a
        >, which is maintained by the Data and Web Science Research Group at the
        University of Mannheim. The corpus comprises over 4.2 million web
        relational tables covering 43
        <a href="https://schema.org/">Schema.org</a> classes. The dataset
        defines both CTA and CPA labels for tables from 17 schema.org
        classes/domains. The authors obtained the CPA labels from Schema.org
        terms used as column headers in a table. The CTA label for a column is
        instead derived from its CPA label using the Schema.org vocabulary
        definition. For the CTA task, the SOTAB dataset provides annotations for
        162,351 columns from 59,548 tables, with a label space of 91 type
        labels. For the CPA task, it provides annotations for 174,998 column
        pairs from 48,379 tables, with a label space of 176 property labels. We
        used the provided train/valid/test splits for both tasks.
      </p>
      <p>
        The WikiTables dataset [<a href="#toc12">4</a>] is a collection of
        tables extracted from the WikiTable corpus [<a href="#toc26">18</a>],
        which contains over 1.54 million tables extracted from Wikipedia pages.
        The dataset defines both CTA and CPA labels. To derive the labels for
        both tasks, the authors referred to Freebase [<a href="#toc28">21</a>]
        to obtain semantic types and relations respectively. For each column,
        the CTA labels are the common types of its entities. For each object
        column in a table, it is paired with the subject column to identify the
        relations shared by more than half of the entity pairs in the two
        columns and thus function as the CPA labels. For the CTA task, the
        dataset provides annotations of 628,254 columns from 397,098 tables,
        with a label space of 255 type labels for training, and 13,025 (13,391)
        columns from 4,764 (4,844) tables for test and validation, respectively.
        For the CPA task, it provides annotations of 62,954 column pairs from
        52,943 tables, with a label space of 121 relation labels for traning,
        and 2,072 (2,175) column pairs from 1,467 (1,560) tables for test
        (validation). The statistics of WikiTable dataset are shown in
        <i>Table 1</i>.
      </p>

      <div style="text-align: center" class="tg-wrap">
        <table class="tg">
          <caption>
            Table 1: Statistics for WikiTable Dataset
          </caption>
          <thead>
            <tr>
              <th class="tg-mkpc"></th>
              <th class="tg-mkpc" colspan="3">CTA</th>
              <th class="tg-mkpc" colspan="3">CPA</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-mkpc"><br /></td>
              <td class="tg-mkpc">mean</td>
              <td class="tg-mkpc">median</td>
              <td class="tg-mkpc">mode</td>
              <td class="tg-mkpc">mean</td>
              <td class="tg-mkpc">median</td>
              <td class="tg-mkpc">mode</td>
            </tr>
            <tr>
              <td class="tg-mkpc">Train columns</td>
              <td class="tg-9wq8">1.58</td>
              <td class="tg-9wq8">1</td>
              <td class="tg-9wq8">1</td>
              <td class="tg-9wq8">2.18</td>
              <td class="tg-9wq8">2</td>
              <td class="tg-9wq8">2</td>
            </tr>
            <tr>
              <td class="tg-mkpc">Val columns</td>
              <td class="tg-9wq8">2.76</td>
              <td class="tg-9wq8">3</td>
              <td class="tg-9wq8">3</td>
              <td class="tg-9wq8">2.39</td>
              <td class="tg-9wq8">2</td>
              <td class="tg-9wq8">2</td>
            </tr>
            <tr>
              <td class="tg-mkpc">Test columns</td>
              <td class="tg-9wq8">2.73</td>
              <td class="tg-9wq8">3</td>
              <td class="tg-9wq8">3</td>
              <td class="tg-9wq8">2.41</td>
              <td class="tg-9wq8">2</td>
              <td class="tg-9wq8">2</td>
            </tr>
          </tbody>
        </table>
      </div>

      <span id="toc4"></span>
      <h2>4. Methodology</h2>
      <p>
        Chapter 4 describes the approaches introduced in this work. We give an
        overview of data preprocessing and augmentations performed, followed by
        serialization methods to account for table content, and hyperparameter
        tuning.
      </p>
      <span id="toc4.1"></span>
      <h3>4.1 Preprocessing and Augmentation</h3>
      <p>
        To accommodate the token limit of 512 tokens for BERT-like language
        models, we trim cell values for longer textual data to fit more cells
        from a table. Following Wang et al. [<a href="#toc16">8</a>]'s empirical
        studies, we retain at most 8 tokens from a cell. This is because long
        textual strings often introduce noise and disrupt the structure of a
        table input. Additionally, we experiment with different threshold cell
        lengths, i.e., <em>median</em> and <em>mean</em> cell length computed
        per column basis. The rational being a long textual column contains
        strings of different length.
      </p>
      <p>
        To enhance our data processing, we perform data augmentation (DA) at
        both the cell and table level during the earlier stages of our workflow.
        Specifically, we apply standard textual DA techniques, such as word
        swapping, word replacement, word deletion, and column cell value
        shuffling, at the cell level. At the table level, we perform random cell
        deletion and column cell shuffling.
      </p>

      <span id="toc4.2"></span>
      <h3>4.2 Serialization</h3>
      <p>
        A Transformer-based language models expect token (text) sequences as
        inputs. This translates to representing tables in text sequences,
        formally known as table serialization, for the tasks of CTA and CPA.
        Single-column serialization is an intuitive solution that concatenates
        column values into a sequence to feed into the language model.
        Specifically, as defined by Suhara et al. [<a href="#toc15">7</a>], for
        column <i>C</i> with column values <i>v<sub>1</sub>,...,v<sub>m</sub></i
        >, the serialized sequence is [<a href="#toc15">7</a>]:
        <span class="math display"
          >\[serialize_{single}(C)::= [CLS]\;v_1\;...\;v_m\;[SEP]\]</span
        >
      </p>
      <p>
        Table serialization conveniently translates the CTA and CPA tasks into
        sequence classification and sequence-pair classification task
        respectively. However, the major drawback of single-column serialization
        is that it treats each column in a table as an independent sequence,
        neglecting the crucial table context that defines a real-world entity
        for a relational table. Previous works [<a href="#toc15">7,</a
        ><a href="#toc17">9,</a><a href="#toc18">10,</a><a href="#toc19">11</a>]
        have highlighted the importance of table context for the CTA task. At
        the other end of the spectrum for table serialization is DODUO, which
        represents a table as an aggregation of contextual information from all
        column values.
      </p>
      <p>
        In this work, we employ two distinct serialization techniques -
        Neighboring Column serialization and TaBERT serialization. In the
        following, we present an explanation of these techniques, followed by an
        example for both serialization as shown in <a id="Fig7">Figure 7</a>.
      </p>

      <p>
        <b>(i) Neighboring Column:</b> The
        <em>Neighboring Column Serialization</em> technique, illustrated in
        <a id="Fig5">Figure 5</a>, utilizes adjacent columns as local context in
        a relational table. Previous research by SATO [<a href="#toc19">11</a>]
        demonstrated that incorporating local context aids in resolving semantic
        type ambiguities in single-column predictions. To accomplish this, their
        approach incorporates the predicted semantic types of immediately
        adjacent columns as local context for a column prediction model of
        neural network. For our approach, we utilize neighboring columns of
        varying window sizes along with the target column in the CTA task.
        Furthermore, for the CPA task, we incorporate neighboring columns for
        both the main and target columns.
      </p>
      <p>
        To build on the Neighboring Column Serialization approach, we introduce
        a more complex variation called
        <em>Neighboring Column with Summary Serialization</em>. This approach
        leverages results from exploratory data analysis (EDA) to extract
        relevant column value data. We use the pandas-profiling library [<a
          href="#toc20"
          >12</a
        >] to infer column data types and analyzes text and context using common
        statistical metrics such as mean, maximum, and minimum text length. We
        prepend the statistical results to the data for each column, providing a
        syntax summary for lengthy columns/tables as part of the local context.
      </p>
      <p>
        However, for the CPA task, incorporating neighboring columns for both
        the main and target columns translates to doubling the number of total
        columns included in the serialization as compared to CTA. Therefore, we
        transfer the Neighboring Column Serialization technique from the CTA
        task to the CPA task, without including the main column, which we refer
        to as the
        <em>No-Main Neighboring Column Serialization</em>.
      </p>

      <span id="toc4.2.1.Fig"></span>
      <figure>
        <img
          src="./images/Neighbor.gif"
          alt="Neighbor illustration GIF"
          style="width: 550px; height: 330px"
          class="center"
        />
        <figcaption class="center">
          <b><a id="Fig5"></a>Figure 5:</b> Neighboring Column Serialization
        </figcaption>
      </figure>

      <p>
        <b>(ii) TaBERT:</b> The <em>TaBERT Serialization</em>, illustrated in
        <a id="Fig6">Figure 6</a>, here uses the row-based idea from the thesis
        by Yin, P et al[<a href="#toc28">20</a>]. To solve the CTA/CPA task, we
        implement the idea in a different way. Our goal is to capture the
        relationship between the target column and the context columns by
        inputting the context data row-wise. To achieve this, we follow the
        approach proposed by TUTA [<a href="#toc16">8</a>], where we prepend a
        data type token to each cell. This row serialization approach helps the
        Transformer's attention mechanism better capture the table structure
        while maintaining constant intervals for each column. Moreover, by
        including data types, the language model can learn the relationships
        between the cells of the target column and the prediction label. For
        instance, a URL data for company, label, or hotel photos may indicate a
        target label of "Logo".
      </p>
      <p>
        To reduce the number of additional tokens required for inserting data
        types in each cell, we introduce <em>Column-TaBERT Serialization</em>,
        which infers the data type for each column by majority count and
        includes the data types for each column only once. For the CTA task, the
        resulting serialization is illustrated in <i>Figure 3</i> or:
        <span class="math display"
          >\[serialize_{col\_tabert}(C)::=
          [CLS]\;target\_col\_type\;target\_col\_data\;[SEP]\;context\_col\_1\_type\;context\_col\_2\_type\;...\;[SEP]\;row\_data\;[SEP]\]</span
        >
      </p>

      <span id="toc4.2.2.Fig"></span>
      <figure>
        <img
          src="./images/TaBERT.gif"
          alt="TaBERT illustration GIF"
          style="width: 550px; height: 330px"
          class="center"
        />
        <figcaption class="center">
          <b><a id="Fig6"></a>Figure 6:</b> TaBERT Serialization
        </figcaption>
      </figure>
      <span id="toc4.2.3.Fig"></span>
      <figure>
        <img
          src="./images/example_table.jpg"
          alt="Example table for serialization"
          style="width: 70%; height: 40%"
          class="center"
        />
        <figcaption class="center">
          <b><a id="Fig7"></a>Figure 7:</b> Illustration of Different
          Serialization Methods
        </figcaption>
      </figure>

      <span id="toc4.3"></span>
      <h3>4.3 Models</h3>
      <p>
        Our approaches are built upon Transformer-based language models. We
        utilize mainly BERT and RoBERTa models and experimentally with the
        LongFormer model for the CPA task.
      </p>
      <p>
        <b>BERT</b> (Bidirectional Encoder Representations from Transformers) is
        a natural language processing model based on Transformers, which was
        proposed by Google <a href="#toc22">[14]</a>. It uses bidirectional
        encoders to learn context-dependent word representations and has
        achieved outstanding performance in various NLP tasks, such as sentiment
        analysis, question answering, and natural language inference.
      </p>
      <p>
        <b>RoBERTa</b> (Robustly Optimized BERT approach) is an improved version
        of the BERT model that was proposed by Facebook AI
        <a href="#toc23">[15]</a>. RoBERTa is pre-trained on a massive amount of
        text data with longer training times and larger batches than BERT, which
        improves its performance in downstream NLP tasks.
      </p>
      <p>
        <b>LongFormer</b> Transformer-based models suffer from a limitation when
        it comes to processing long sequences which is a challenge in table
        annotation where long texts need to be processed. While models such as
        BERT or RoBERTa [<a href="#toc22">14</a>, <a href="#toc23">15</a>] are
        limited to a sequence length of 512, [<a href="#toc21">13</a>] proposes
        Longformer, which has a linear attention mechanism that can process
        longer sequences length. The maximum length is constrained by the memory
        of GPU.
      </p>
      <p>
        We further explore different experimental models for the CPA task in
        various iterations.
      </p>
      <p>
        <b>(i) Subtable Model:</b> According to the statistics provided by SOTAB
        Benchmark [<a href="#toc9">1</a>], the CPA task involves 48,379 tables
        with a total of 174,998 columns. Notably, the tables have a median of 42
        rows and 8 columns. To increase the diversity of the training data and
        incorporate more parts of each table, we divide each table into
        subtables consisting of a maximum of 20 rows per subtable and up to 5
        subtables per table. As a result, our approach now encompasses a total
        of 391,115 columns.
      </p>
      <p>
        <b>(ii) 2-Step Model:</b> To improve the accuracy of our predictions, we
        adopt a 2-step model that leverages the 17 schema.org labels associated
        with each table. Specifically, we first predict the schema type of a
        table using a model trained on the SOTAB Benchmark [<a href="#toc9">1</a
        >] training data. This allows us to restrict our predictions to only
        relevant labels for each column. For example, if a column is identified
        as "Book", it will not be predicted as "MusicRecording/Name" for a CTA
        task. Next, we train submodels for each schema type to predict the final
        label for each column. To make predictions on test data, we first
        predict the schema type and then use the corresponding submodel to
        predict the final label for each column.
      </p>
      <span id="toc4.4"></span>
      <h3>4.4 Hyperparameter Tuning</h3>
      <p>
        To investigate the impact of different window sizes for Neighboring
        Column Serializations, we conduct hyperparameter tunings for both CTA
        and CPA tasks. We also experimented with different preprocessing
        methods, such as TUTA, median, and mean preprocessing for each promising
        serialization. Additionally, we varied the proportions of the target
        column and local context <i>(MP - Main Percentange Ratio)</i> to assess
        the effect of context weight on different serializations, experimenting
        with MP Ratios of 0.2, 0.5, and 0.8, respectively. For instance, the MP
        Ratio of 0.8 for Neighboring Column Serialization, for the CPA task,
        allocates 80% of the available token limit to the Main and Target
        Columns and only 20% to their neighbors, Finally we investigate the
        performance of different augmentationstrategies
      </p>

      <span id="toc5"></span>
      <h2>5. Experimental Approach and Evaluation</h2>
      <span id="toc5.1"></span>
      <h3>5.1 Setup</h3>
      <p>
        For our experiments on both the SOTAB benchmark and WikiTables dataset,
        we trained a RoBERTa model with default settings: 30 epochs, 3 runs
        (with seed 0-2), and a batch size of 32, if not specified. The initial
        learning rate was set to 5e-5, and we used a linear decay scheduler with
        no warmup. We set a maximum token length of 512, used a window size of
        5, and a standard MP ratio of 0.5 for relevant serializations. We
        identify single-column serialization as the baseline configuration. For
        the SOTAB benchmark, we formulated the task as a multi-class prediction
        problem and used Cross Entropy loss. For the WikiTable dataset, we
        formulated the task as a multi-label prediction problem and used Binary
        Cross Entropy loss. We evaluated the performance of our models using
        micro F1 scores for both datasets.
      </p>
      <p>
        To conduct our experiments, we utilized computing resources from the
        University of Mannheim (dws-server) and bwHPC servers provided by the
        state of Baden-Wrttemberg.
      </p>
      <span id="toc5.2"></span>
      <h3>5.2 Experiments for SOTAB benchmark</h3>
      <p>
        For the SOTAB benchmark, we conducted extensive experiments on
        serialization for both CTA and CPA tasks. Specifically, we tested
        different variants of Neighboring Column Serialization and TaBERT
        Serialization. For CPA, we also performed No-Main Neighboring Column
        Serialization, which treats a CPA task as a CTA task.
      </p>
      <p>
        In order to explore more possibilities for the challenging CPA task, we
        further investigated Subtable, 2-Step, and Longformer models. For the
        Subtable model, we trained a BERT model for 30 epochs with a batch size
        of 16 and a maximum token length of 512 tokens. We utilized a
        Neighboring Column Serialization of window size 2 for this model. For
        the 2-Step model, we used default parameter settings, including a
        RoBERTa model trained for 30 epochs. For the Longformer model, we used
        single-column serialization with a batch size of 8 and maximum length of
        1000.
      </p>

      <span id="toc5.3"></span>
      <h3>5.3 Experiments for WikiTables dataset</h3>
      <p>
        To test the generalizability of our findings for the SOTAB benchmark, we
        transferred the serialization methods to the WikiTables dataset. For the
        CTA task, we used only Column-TaBERT Serialization due to computational
        limitations resulting from the dataset's size. For the CPA task, we
        experimented with different serializations, including TaBERT,
        Column-TaBERT, and No-Main Neighboring Column Serializations. We
        utilized the default experimental parameters as described and following
        Suhara et al. [<a href="#toc15">7</a>], we trained the model for 15
        epochs.
      </p>

      <span id="toc5.4"></span>
      <h3>5.4 Main Results</h3>
      <p>
        <b> SOTAB benchmark </b>
      </p>
      <p>
        <em>Table 2</em> reports the results for SOTAB benchmark. All
        serialization methods except DODUO and TURL utilize the RoBERTa model.
        Across both tasks, for all but one serializations implemented, we
        achieved significant improvement on Micro F1 against the single-column
        baseline serialization. For the CTA task, we are able to achieve a
        maximum increase of 7.62 point F1 score compared to the state-of-the-art
        DODUO model, with Neighboring Column Serialization of window size of 5
        and a MP Ratio of 0.2. Similar results are achieved by TaBERT and
        Neighoring Column Serialization indicating the importance of context
        ratio and certain structure for input sequence. The lower F1 score of
        Column-TaBERT compared to its counterpart of TaBERT, and like-wise for
        Neighboring Column with Summary, by roughly 3 points suggests the
        inability of Transformer's attention mechanism to identify significant
        positions in a input sequence.
      </p>
      <p>
        For the CPA task, different variances of Neighboring Column
        Serialization achieve 6 point higher F1 score than the existing methods.
        Once again, Neighboring Column Serialization of window size of 5 and MP
        Ratio of 0.2 achieves the highest F1 score. TaBERT variances score 2.5
        point lower F1 score indicating less suitability of row serialization
        for a column pair prediction. Once again, less structured serialization
        solutions achieve lower F1 scores (Column TaBERT and Neighboring Column
        with Summary). Lastly, only Neighboring Column with Summary with MP
        Ratio of 0.8 achieves lower F1 score than Single-Column. Overall, our
        results highlight the importance of carefully selecting the appropriate
        serialization method to achieve optimal performance in table annotation
        tasks.
      </p>

      <p>
        Longformer's performance in comparison to BERT is lower, and one
        possible reason for this is that the Longformer model employs a
        different attention mechanism, which may not converge well with the
        current set of parameters
      </p>
      <p>
        For the additional experiments, we did not achieve improvement on Test
        F1 score. Notably for LongFormer model, further modifications on the
        model setup should be investigated. For the 2-step model, we achieve a
        Test F1 Score of 96.73 for the schema type prediction model. Building on
        top of this model, the final ensembly of submodels by schema class
        achieved a Test F1 Score of 84.34. Upon further investigating,
        decoupling the first model and inserting test data with correct schema
        labels into respective submodel merely increased the final Test F1 score
        to 84.81.
      </p>
      <div style="text-align: center" class="tg-wrap">
        <table class="tg tg-top">
          <thead>
            <caption>
              Table 2a: CTA Experiment results
            </caption>
            <tr>
              <th class="tg-mkpc"></th>
              <th class="tg-mkpc" colspan="3">
                <span style="background-color: #f0f0f0">MP Ratio</span>
              </th>
            </tr>
          </thead>
          <tfoot>
            <tr>
              <td colspan="4">
                <sup>*</sup>: MP Ratio is irrelevant; <sup>**</sup>: Results
                from [<a href="#toc9">1</a>]
              </td>
            </tr>
          </tfoot>
          <tbody>
            <tr>
              <th class="tg-syo5">CTA</th>
              <th class="tg-syo5">0.2</th>
              <th class="tg-syo5">0.5</th>
              <th class="tg-syo5">0.8</th>
            </tr>
            <tr>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff"
                  >Single-Column<sup>*</sup></span
                >
              </td>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff">-</span>
              </td>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff">82.97</span>
              </td>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff">-</span>
              </td>
            </tr>
            <tr>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff"
                  >DODUO<sup>*,**</sup></span
                >
              </td>
              <td class="tg-9wq8">
                <span style="font-weight: 400; font-style: normal">-</span>
              </td>
              <td class="tg-9wq8">
                <span style="font-weight: 400; font-style: normal">84.82</span>
              </td>
              <td class="tg-9wq8">
                <span style="font-weight: 400; font-style: normal">-</span>
              </td>
            </tr>
            <tr>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff"
                  >TURL<sup>*,**</sup></span
                >
              </td>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff">-</span>
              </td>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff">78.96</span>
              </td>
              <td class="tg-9wq8">
                <span style="color: #333; background-color: #fff">-</span>
              </td>
            </tr>
            <tr>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >TaBERT</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >92.14</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >92.24</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >91.60</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Col-TaBERT</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >87.92</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >88.40</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >88.67</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Neighbor, WS=5</span
                >
              </td>
              <td class="tg-p91v">
                <span
                  style="
                    font-weight: 700;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >92.44</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >92.14</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >91.29</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Sum_Neighbor, WS=5</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >90.97</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >88.63</span
                >
              </td>
              <td class="tg-9wq8">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.82</span
                >
              </td>
              <tfoot>
                <tr></tr>
              </tfoot>
            </tr>
          </tbody>
        </table>

        <table class="tg">
          <caption>
            Table 2b: CPA Experiment results
          </caption>
          <thead>
            <tr>
              <th class="tg-mkpc"></th>
              <th class="tg-mkpc" colspan="3">
                <span style="background-color: #f0f0f0">MP Ratio</span>
              </th>
            </tr>
          </thead>
          <tfoot>
            <tr>
              <td colspan="4">
                <sup>*</sup>: MP Ratio is irrelevant; <sup>**</sup>: Results
                from [<a href="#toc9">1</a>]
              </td>
            </tr>
          </tfoot>
          <tbody>
            <tr>
              <th class="tg-syo5">CPA</th>
              <th class="tg-syo5">0.2</th>
              <th class="tg-syo5">0.5</th>
              <th class="tg-syo5">0.8</th>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Single-Column<sup>*</sup></span
                >
              </td>
              <td class="tg-zkss">-</td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >79.52</span
                >
              </td>
              <td class="tg-h2b0">-</td>
            </tr>

            <tr>
              <td class="tg-h2b0">DODUO<sup>*,**</sup></td>
              <td class="tg-h2b0">-</td>
              <td class="tg-h2b0">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >79.96</span
                >
              </td>
              <td class="tg-h2b0">-</td>
            </tr>
            <tr>
              <td class="tg-c3ow">TURL<sup>*,**</sup></td>
              <td class="tg-c3ow">-</td>
              <td class="tg-c3ow">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >72.93</span
                >
              </td>
              <td class="tg-c3ow">-</td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >TaBERT</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >83.09</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >83.68</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >82.49</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Col-TaBERT</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >82.79</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >83.22</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >82.38</span
                >
              </td>
            </tr>

            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Neighbor, WS=5</span
                >
              </td>
              <td class="tg-p91v">
                <span
                  style="
                    font-weight: 700;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >86.09</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.50</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.14</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Sum_Neighbor, WS=5</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >83.50</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >82.07</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >77.82</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >No-Main Neighbor, WS=5</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.88</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.29</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >84.87</span
                >
              </td>
            </tr>
          </tbody>
        </table>
        <table class="tg">
          <caption>
            Table 2c: Further Experiments for CPA
          </caption>
          <thead>
            <tr>
              <th class="tg-5vuh"></th>
              <th class="g-4akn">
                <span
                  style="
                    font-weight: 700;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Test Micro F1</span
                >
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-zkss">Subtable Model*</td>
              <td class="tg-zkss">83.20</td>
            </tr>
            <tr>
              <td class="tg-zkss">Two-Step Model*</td>
              <td class="tg-zkss">84.34</td>
            </tr>
            <tr>
              <td class="tg-zkss">Longformer*</td>
              <td class="tg-zkss">51.74</td>
            </tr>
          </tbody>
        </table>
      </div>

      <p>
        <b> WikiTables Dataset </b>
      </p>
      <p>
        <em>Table 3</em> reports the results for WikiTables Dataset. Similarly
        to the SOTAB benchmark, all serialization methods utilize the RoBERTa
        model except for DODUO and TURL, which we report results measured in [<a
          href="#toc9"
          >1</a
        >]. In the CTA task, our Column-TaBERT Serialization achieved a
        comparable F1 score to the state-of-the-art DODUO, missing by only 1
        point. This is a significant improvement over the TURL baseline, which
        we outperformed by 2.5 points. However, our results from SOTAB benchmark
        suggest that there may be rooms for improvement using more performant
        serialization methods. For the CPA task, our TaBERT Serialization
        outperformed existing methods by 1.5 points in terms of F1 score. These
        results demonstrate that our serialization approaches can achieve
        comparable performance on different real-world datasets. Overall, our
        findings suggest that careful selection of serialization methods can
        significantly improve the performance of table annotation tasks.
      </p>
      <div style="text-align: center" class="tg-wrap">
        <table class="tg">
          <caption>
            Table 3a: CTA result for WikiTable
          </caption>
          <thead>
            <tr>
              <th class="tg-5vuh"><br /></th>
              <th class="tg-4akn">
                <span
                  style="
                    font-weight: 700;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Test Micro F1</span
                >
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Col-TaBERT</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >91.42</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >DODUO</span
                >
              </td>
              <td class="tg-p91v">
                <span
                  style="
                    font-weight: 700;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >92.45</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >TURL</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >88.86</span
                >
              </td>
            </tr>
          </tbody>
        </table>
        <table class="tg">
          <caption>
            Table 3b: CPA result for WikiTable
          </caption>
          <thead>
            <tr>
              <th class="tg-5vuh"><br /></th>
              <th class="tg-4akn">
                <span
                  style="
                    font-weight: 700;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Test Micro F1</span
                >
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >Col-TaBERT</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >92.85</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >DODUO</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >91.72</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >TURL</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >90.94</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >TaBERT</span
                >
              </td>
              <td class="tg-p91v">
                <span
                  style="
                    font-weight: 700;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >93.30</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >No-Main Neighbor</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >91.60</span
                >
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <span id="toc5.5"></span>
      <h3>5.5 Results of Different Hyperparameters</h3>
      <p>
        This section describes the hyperparameter tuning results for selected
        serializations. We conducted preprocessing of fixing cell length in a
        tables, implementing different data augmentation techniques, altering
        the window size for Neighboring Column serialization and changing the MP
        Ratio.
      </p>
      <p>
        <b> Preprocess and Augmentation</b>
      </p>
      <p>
        Through our experiments, as shown in <em>Table 4a</em>, we found that
        preprocessing provides slight improvement on the performance of
        different serialization methods. This highlights the importance of
        providing structured input sequences to Transformer models and using
        more columns whenever possible. Furthermore, our initial experiments
        with data augmentation using single-column serialization indicate that,
        for large datasets like the SOTAB benchmark, the need for data
        augmentation may be limited. This is reported in <em>Table 4c</em>.
      </p>

      <p>
        <b> Window Size</b>
      </p>

      <p>
        For the SOTAB benchmark, we identified that the suitable window size for
        Neighboring Column Serializations are window size of 4 - 6, as shown in
        <em>Table 4b</em>. This indicates that a complete table structure with
        relevant columns is a suitable input for language models in table
        annotation tasks. Notably, we observed that the two main differences
        between Neighboring Column Serialization and DODUO are the structure of
        the input sequence and the pretrained language model used .
        Specifically, our approach places the target column at the front of the
        input sequence to focus on it, while DODUO takes the entire table as
        input. For model, We used RoBERTa model instead of BERT model used by
        DODUO.
      </p>

      <p>
        <b> MP Ratio</b>
      </p>
      MP (Main Percentage Ratio) played significant value for certain
      serializations such as Neighboring Column with Summary serialization.
      However, the results in <em>Table 2</em> confirms that different
      serialization requires different MP Ratio due to the nature of its
      serialization strategy.
      <p></p>
      <div style="text-align: center" class="tg-wrap">
        <table class="tg">
          <caption>
            Table 4a: Preprocess and Augmentation test
          </caption>
          <thead>
            <tr>
              <th class="tg-4akn">
                <span
                  style="
                    font-weight: bold;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >Preprocessing Method</span
                >
              </th>
              <th class="tg-4akn">
                <span
                  style="
                    font-weight: bold;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >CTA with TaBERT</span
                ><br /><span
                  style="
                    font-weight: bold;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >MP Ratio=0.5</span
                >
              </th>
              <th class="tg-4akn">
                <span
                  style="
                    font-weight: bold;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                >
                  CPA with No-Main Neighbor</span
                ><br /><span
                  style="
                    font-weight: bold;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >MP Ratio=0.5</span
                >
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >TUTA</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >92.45</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >85.77</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >MEAN</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >92.05</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >85.70</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >MEDIAN</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >92.24</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >85.68</span
                >
              </td>
            </tr>
          </tbody>
        </table>
        <table class="tg">
          <caption>
            Table 4b: Window Size test for Neighboring Column
          </caption>
          <thead>
            <tr>
              <th class="tg-syo5">Window Size</th>
              <th class="tg-syo5">CTA</th>
              <th class="tg-syo5">CPA</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >1</span
                >
              </td>
              <td class="tg-3xi5">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >89.59</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >83.31</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >2</span
                >
              </td>
              <td class="tg-3xi5">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >91.20</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >84.71</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >3</span
                >
              </td>
              <td class="tg-3xi5">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >92.09</span
                >
              </td>
              <td class="tg-4akn">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >85.31</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >4</span
                >
              </td>
              <td class="tg-3xi5">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >92.21</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.86</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >5</span
                >
              </td>
              <td class="tg-3xi5">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >92.14</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.50</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >6</span
                >
              </td>
              <td class="tg-3xi5">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >91.80</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.79</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >7</span
                >
              </td>
              <td class="tg-3xi5">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >91.78</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                    background-color: transparent;
                  "
                  >85.81</span
                >
              </td>
            </tr>
          </tbody>
        </table>
        <table class="tg">
          <caption>
            Table 4c: Augmentation method on single-column serialization
          </caption>
          <thead>
            <tr>
              <th class="tg-mkpc">
                <span
                  style="font-style: normal; text-decoration: none; color: #000"
                  >Augmentation method</span
                >
              </th>
              <th class="tg-mkpc">
                <span
                  style="font-style: normal; text-decoration: none; color: #000"
                  >Test Micro F1</span
                >
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >Baseline</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >77.95</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >Delete Random Cell</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >77.28</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >Replace with Frequent Value</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >78.00</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >Shuffle Column Cell</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >77.29</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >Shuffle Column Cell Value</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >77.05</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >Swap Word</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >74.13</span
                >
              </td>
            </tr>
            <tr>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >Replace Word</span
                >
              </td>
              <td class="tg-zkss">
                <span
                  style="
                    font-weight: 400;
                    font-style: normal;
                    text-decoration: none;
                    color: #000;
                  "
                  >75.48</span
                >
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <span id="toc5.6"></span>
      <h3>5.6 Error Analysis</h3>
      <p>
        To evaluate the performance, we conducted error analysis on the
        Neighboring Column Serialization with window size of 2. We chose this
        approach because it represents the promising serialization method. The
        parameters utilized is a BERT model trained for 30 epochs with batch
        size of 16. We initially conducted statistic analysis on three different
        aspects: (i) schema type, (ii) data type, and (iii) specific challenges.
        Subsequently, we further looked into the data and manually classified
        the errors.
      </p>
      <p>
        <b>(i) Schema Type: </b> The tables are distributed across 17 schema.org
        types and the tables do no include any metadata which means no table
        headers or cpations are available. We list the top 5 labels that
        achieved the highest score in the overall test set and the top 5 labels
        that reached the lowest score for CTA in <i>Table 5a</i> and CPA in
        <i>Table 5b</i>. The performance of CTA and CPA varies across different
        schema types. <strong>MusicRecording</strong> is the only schema where
        both tasks perform well, while
        <strong>CreativeWork and Product</strong> are weak for both tasks.
        <strong>Recipe and Person</strong> are among the top 5 schemas for CTA,
        but are among the lowest 5 for CPA.
      </p>
      <p>
        We attribute this discrepancy to the fact that CPA employs more
        comprehensive and detailed classification labels. For instance, the
        Recipe schema in CPA includes specific labels for fatContent,
        sugarContent, and proteinContent, whereas in CTA, they are all labeled
        as Mass. Additionally, cookingTime and prepTime in CPA are classified as
        Duration in CTA. In Person schema, the CTA tasks are more
        straightforward such as Person/name, telephone, and Country. while CPA
        has more complicated tasks like affiliation, memberOf, worksFor,
        alumniOf which can be ambiguous.
      </p>

      <div style="text-align: center" class="tg-wrap">
        <table class="tg">
          <caption>
            Table 5a: Schema Type result for CTA
          </caption>
          <thead>
            <tr>
              <th class="tg-ixdq" colspan="2">Top 5</th>
              <th class="tg-ixdq" colspan="2">Lowest 5</th>
            </tr>
            <tr>
              <th class="tg-mkpc">Label</th>
              <th class="tg-mkpc">Overall Test F1</th>
              <th class="tg-mkpc">Label</th>
              <th class="tg-mkpc">Overall Test F1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Recipe</td>
              <td>96.23</td>
              <td>Product</td>
              <td>86.54</td>
            </tr>
            <tr>
              <td>Museum</td>
              <td>95.40</td>
              <td>Hotel</td>
              <td>86.05</td>
            </tr>
            <tr>
              <td>Event</td>
              <td>93.89</td>
              <td>JobPosting</td>
              <td>84.14</td>
            </tr>
            <tr>
              <td>Person</td>
              <td>93.07</td>
              <td>CreativeWork</td>
              <td>82.50</td>
            </tr>
            <tr>
              <td>MusicRecording</td>
              <td>92.59</td>
              <td>MusicAlbum</td>
              <td>74.07</td>
            </tr>
          </tbody>
        </table>

        <table class="tg">
          <caption>
            Table 5b: Schema Type result for CPA
          </caption>
          <thead>
            <tr>
              <th class="tg-ixdq" colspan="2">Top 5</th>
              <th class="tg-ixdq" colspan="2">Lowest 5</th>
            </tr>
            <tr>
              <th class="tg-mkpc">Label</th>
              <th class="tg-mkpc">Overall Test F1</th>
              <th class="tg-mkpc">Label</th>
              <th class="tg-mkpc">Overall Test F1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Restaurant</td>
              <td>96.25</td>
              <td>SportsEvent</td>
              <td>84.50</td>
            </tr>
            <tr>
              <td>TVEpisode</td>
              <td>96.08</td>
              <td>Recipe</td>
              <td>82.25</td>
            </tr>
            <tr>
              <td>MusicRecording</td>
              <td>94.64</td>
              <td>Person</td>
              <td>81.24</td>
            </tr>
            <tr>
              <td>Place</td>
              <td>93.18</td>
              <td>CreativeWork</td>
              <td>76.36</td>
            </tr>
            <tr>
              <td>LocalBusiness</td>
              <td>90.04</td>
              <td>Product</td>
              <td>70.34</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>
        <b>(ii) Data Type: </b> The columns are divided into 5 groups based on
        their data type: Boolean, Numeric, URL, Short Text and Long Text.
        Boolean columns contain only "yes" or "no" or "true" or "false" values,
        Numeric columns contain only numbers, and URL columns contain web
        addresses. All other columns are classified as text, and if the token in
        the value is less than five, it is considered Short Text, otherwise, it
        is Long Text. The results of each data type with micro-F1 score and
        distribution are summarized in <i>Table 6a</i> for CTA and
        <i>Table 6b</i> for CPA.
      </p>
      <p>
        We observed that the <strong>Boolean</strong> data accounts for a very
        small portion of the overall test sets. Therefore, any predictions made
        on this data can significantly impact the final score. For instance, in
        CPA task, although there were only 3 incorrect predictions out of a
        total of 20 Boolean data, the score decreased notably. Upon further
        investigation, we found these errors were due to wrong truth labels of
        boolean data as height or gtin8. When comparing the CPA and CTA tasks in
        other data types, it appears that the CPA tasks have lower scores. This
        could be due to the fact that the CPA tasks involve splitting the label
        spaces further, which can make it more challenging to accurately assign
        the correct labels. For instance, in <strong>Numeric</strong> data, the
        CPA tasks have large numbers of incorrect predictions between gtin,
        gtin12, gtin13 and gtin14. Also in <strong>URL</strong> data, some URLs
        refer to the source of images. In CPA tasks, these URLs are further
        split into more specific categories such as logo, image, and photo which
        have high degree of similarity and make it more difficult for the model
        to distinguish between them. The same issue also applies to
        <strong>Short Text</strong> and <strong>Long Text</strong> data types.
      </p>
      <div style="text-align: center" class="tg-wrap">
        <table class="tg">
          <caption>
            Table 6a: Data Type result for CTA
          </caption>
          <thead>
            <tr>
              <th class="tg-mkpc">Data Type</th>
              <th class="tg-mkpc">Overall Test F1</th>
              <th class="tg-mkpc">Percentage of the type</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Boolean</td>
              <td>100</td>
              <td>0.3</td>
            </tr>
            <tr>
              <td>Numeric</td>
              <td>91.77</td>
              <td>4.7</td>
            </tr>
            <tr>
              <td>URL</td>
              <td>97.06</td>
              <td>6.8</td>
            </tr>
            <tr>
              <td>Long Text</td>
              <td>86.92</td>
              <td>18.8</td>
            </tr>
            <tr>
              <td>Short text</td>
              <td>90.27</td>
              <td>69.4</td>
            </tr>
          </tbody>
        </table>

        <table class="tg">
          <caption>
            Table 6b: Data Type result for CPA
          </caption>
          <thead>
            <tr>
              <th class="tg-mkpc">Data Type</th>
              <th class="tg-mkpc">Overall Test F1</th>
              <th class="tg-mkpc">Percentage of the type</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Boolean</td>
              <td>85</td>
              <td>0.1</td>
            </tr>
            <tr>
              <td>Numeric</td>
              <td>75.06</td>
              <td>7.8</td>
            </tr>
            <tr>
              <td>URL</td>
              <td>94.71</td>
              <td>4.5</td>
            </tr>
            <tr>
              <td>Long Text</td>
              <td>88.17</td>
              <td>12.5</td>
            </tr>
            <tr>
              <td>Short Text</td>
              <td>82.52</td>
              <td>75.2</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>
        <b>(iii) Specific Challenges: </b>The specific challgenges include:
        Missing Values, Value Format Heterogeneity, Corner Cases, and Random
        Columns which were all clearly defined in
        <a href="https://webdatacommons.org/structureddata/sotab/"
          >WDC Schema.org Table Annotation Benchmark (SOTAB)</a
        >[<a href="#toc9">1</a>]. The results are summarized in
        <i>Table 7</i> for CTA and CPA.
      </p>
      <div style="text-align: center" class="tg-wrap">
        <table class="tg">
          <caption>
            Table 7: Specific Challenges Results for CTA & CPA
          </caption>
          <thead>
            <tr>
              <th></th>
              <th class="tg-ixdq">CTA</th>
              <th class="tg-ixdq">CPA</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-lboi">Test (Missing Values)</td>
              <td class="tg-lboi">
                <span style="font-weight: 400; font-style: normal"></span>90.60
              </td>
              <td class="tg-lboi">
                <span style="font-weight: 400; font-style: normal"></span>82.74
              </td>
            </tr>
            <tr>
              <td class="tg-lboi">Test (Format Heterogeneity)</td>
              <td class="tg-lboi">
                <span style="font-weight: 400; font-style: normal"></span>97.09
              </td>
              <td class="tg-lboi">
                <span style="font-weight: 400; font-style: normal"></span>86.55
              </td>
            </tr>
            <tr>
              <td class="tg-lboi">Test (Corner Cases)</td>
              <td class="tg-lboi">
                <span style="font-weight: 400; font-style: normal"></span>85.06
              </td>
              <td class="tg-lboi">
                <span style="font-weight: 400; font-style: normal"></span>73.48
              </td>
            </tr>
            <tr>
              <td class="tg-lboi">Test (Random Columns)</td>
              <td class="tg-lboi">
                <span style="font-weight: 400; font-style: normal"></span>90.89
              </td>
              <td class="tg-lboi">
                <span style="font-weight: 400; font-style: normal"></span>85.36
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <p>
        <b>Error Sampling</b>
      </p>
      <p>
        Following the statistical analysis of our results, we delved deeper into
        the data itself to investigate the root causes of the errors. To do
        this, we sampled errors in prediction by schema for both the CTA and CPA
        tasks. Specifically, we sampled 614 errors from a total of 1533
        observations for CTA, and 617 errors from 3853 observations for CPA. We
        manually checked these tables and classified the types of errors we
        found, which are listed in <i>Table 8</i>. And the distribution of error
        types in the samples are illustrated in <i>Figure 5</i>.
      </p>
      <div style="text-align: center; text-align: left" class="tg-wrap">
        <table>
          <caption>
            Table 8: Error Types and Definitions
          </caption>
          <thead>
            <th>Observed Error Types</th>
            <th>Definition</th>
            <th>Example</th>
          </thead>
          <tbody>
            <tr>
              <td>Actual wrong label</td>
              <td>The label given in the table is obviously incorrect.</td>
              <td>10-digit-number is labeled as gtin8</td>
            </tr>
            <tr>
              <td>Bad prediction</td>
              <td>
                The data provided by the column and neighbor columns is
                sufficient but the prediction is incorrect.
              </td>
              <td>gtin12 is predicted as gtin14</td>
            </tr>
            <tr>
              <td>Schema related label</td>
              <td>
                The prediction correctly represents the data but does not match
                the associated schema.
              </td>
              <td>In Movie schema, Movie/name is predicted as Book/name</td>
            </tr>
            <tr>
              <td>Semantic label</td>
              <td>
                The prediction has the same data type or format as label, but
                the interpretation of the data was incorrect.
              </td>
              <td>FAXnumber is predicted as telephone number and vice versa</td>
            </tr>
            <tr>
              <td>Duplicate label</td>
              <td>
                Two columns in a table that have same values but are labeled
                differently, leading to data redundancy and potential confusion
                or inconsistency in the interpretation of the data. In this
                case, usually the wrong prediction is the label of the other
                column.
              </td>
              <td>
                columns with same value are labeled as Brand and Organization
                respectively
              </td>
            </tr>
            <tr>
              <td>Hierarchical label</td>
              <td>
                The prediction may represent a higher or lower level of
                abstraction than the label, but the semantic meaning is still
                understandable.
              </td>
              <td>
                <ul>
                  <li>Book/name is predicted as CreativeWork</li>
                  <li>Restaurant is predicted as LocalBusiness</li>
                </ul>
              </td>
            </tr>
            <tr>
              <td>Bad tables</td>
              <td>
                Approximately 80% of the values in the table are either empty
                strings, 'undefined', or contain erroneous data inputted by
                humans.
              </td>
              <td>Example given in <a id="Fig8a">Figure 8a</a>.</td>
            </tr>
            <tr>
              <td>Lack of info</td>
              <td>
                The data provided is insufficient in current and neighbor
                columns but contained in columns outside the windows.
              </td>
              <td>
                Example given in <a id="Fig8b">Figure 8b</a>. In CTA task, the
                target is to predict column 12, while the useful information is
                included in first few columns.
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <div style="text-align: center" class="tg-wrap">
        <figure class="left">
          <img
            src="images/example_bad_tables.png"
            alt="example_bad_tables"
            width="820"
          />
          <figcaption>
            <b><a id="Fig8a"></a>Figure 8a: </b>Example of bad tables
          </figcaption>
        </figure>

        <figure class="right">
          <img
            src="images/example_lack_info.png"
            alt="example_lack_info"
            width="820"
          />
          <figcaption>
            <b><a id="Fig8b"></a>Figure 8b: </b>Example of Lack of information
          </figcaption>
        </figure>
        <figure>
          <img
            src="images/Percentage of Error Type in CTA sample.png"
            alt="percentage of CTA error type"
            width="45%"
          />
          <img
            src="images/Percentage of Error Type in CPA sample.png"
            alt="percentage of CPA error type"
            width="45%"
          />
          <figcaption>
            <b><a id="Fig9"></a>Figure 9:</b> CTA & CPA error types distribution
            in samples.
          </figcaption>
        </figure>
      </div>
      <p>
        <b> Relative Frequency Definition </b>
      </p>
      <p>
        In addition to the absolute count of errors, we also aim to investigate
        the relative frequencies of errors and correct predictions, to determine
        corner cases. Therefore, we seek to compare the ratio of incorrect
        predictions to correct predictions based on the same label.
      </p>
      <span class="math display">
        \[Relative Frequency(P,T) = \frac{P}{{T}}\]
      </span>
      <p>
        where <i>P</i> is counts of prediction for specific (label, prediction)
        pair, and <i>T</i> is counts of prediction of total pairs of the label.
        For example, suppose we have a total of 100 predictions made for the
        'ProductModel' label, of which 20 were incorrectly predicted as
        'IdentifierAT' and 60 were correctly predicted as 'ProductModel',
        Therefore, the Relative Frequency of error prediction pair of
        ('ProductModel', 'IdentifierAT') is 0.2 and the Relative Frequency of
        correct prediction pair of ('ProductModel', 'ProductModel') is 0.6. We
        calculated the frequency on all prediction, and listed the top 5 pairs
        with the highest error frequency in <i>Table 9a</i> for CTA and
        <i>Table 9b</i> for CPA. We also cross-checked those pairs in our
        samples above-mentioned to determine the error types of each pair. Pairs
        might have multiple possible error types depending on the table being
        used.
      </p>
      <p>
        <b>Error Frequency for CTA </b>
      </p>
      <p>
        Upon analyzing CTA's Top 5 errors, as shown in <em>Table 9a</em>, we
        observed that the majority were related to Hierarchical Labels, which
        was consistent with the sampled data. We believe that refining the
        2-step model could address this issue. Despite the model's overall
        effectiveness in capturing the meaning of predicted values, inaccuracies
        persisted, particularly with the addressRegion and addressLocality pair.
        We discovered that while the schema defines addressRegion as the
        country's region, addressLocality corresponds to a city within that
        region. Unfortunately, we encountered numerous inconsistent labeling and
        mixed-up terms in tables. Therefore, future work must focus on
        preprocessing to address this problem by handling inconsistent labels.
      </p>
      <div style="text-align: center" class="tg-wrap">
        <table class="tg">
          <caption>
            Table 9a: Top 5 Error Frequency for CTA
          </caption>
          <thead>
            <tr>
              <th class="tg-mkpc">Label</th>
              <th class="tg-mkpc">Prediction</th>
              <th class="tg-mkpc">Pair count</th>
              <th class="tg-mkpc">Total count</th>
              <th class="tg-mkpc">Error Frequency</th>
              <th class="tg-mkpc">Correct Frequency</th>
              <th class="tg-mkpc">Possible error types</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>MusicRecording</td>
              <td>MusicRecording/name</td>
              <td>4</td>
              <td>14</td>
              <td>0.286</td>
              <td>0.571</td>
              <td>Hierarchical label</td>
            </tr>
            <tr>
              <td>ProductModel</td>
              <td>IdentifierAT</td>
              <td>60</td>
              <td>268</td>
              <td>0.224</td>
              <td>0.496</td>
              <td>Hierarchical label</td>
            </tr>
            <tr>
              <td>faxNumber</td>
              <td>telephone</td>
              <td>34</td>
              <td>172</td>
              <td>0.198</td>
              <td>0.791</td>
              <td>Semantic label</td>
            </tr>
            <tr>
              <td>addressRegion</td>
              <td>addressLocality</td>
              <td>43</td>
              <td>221</td>
              <td>0.195</td>
              <td>0.738</td>
              <td>Duplicate label, Bad prediction, Actual wrong label</td>
            </tr>
            <tr>
              <td>MusicGroup</td>
              <td>MusicArtistAT</td>
              <td>2</td>
              <td>11</td>
              <td>0.182</td>
              <td>0.818</td>
              <td>Hierarchical label</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>
        <b>Error Frequency for CPA </b>
      </p>
      <p>
        In CPA's top 5, shown in <em>Table 9a</em>, the most common error type
        was related to semantic labeling, which highlights the challenges in
        accurately identifying and labeling the meaning of data values.
        Consistent data formats further complicate this issue, making it
        difficult for even humans to distinguish between them without contextual
        information. To address this issue, we propose including additional
        relevant columns, such as information about the stadium, to disambiguate
        between pairs of data like awayTeam and homeTeam, which can potentially
        improve the accuracy of the model's predictions.
      </p>

      <div style="text-align: center" class="tg-wrap">
        <table class="tg">
          <caption>
            Table 9b: Top 5 Error Frequency for CPA
          </caption>
          <thead>
            <tr>
              <th class="tg-mkpc">Label</th>
              <th class="tg-mkpc">Prediction</th>
              <th class="tg-mkpc">Pair count</th>
              <th class="tg-mkpc">Total count</th>
              <th class="tg-mkpc">Error Frequency</th>
              <th class="tg-mkpc">Correct Frequency</th>
              <th class="tg-mkpc">Possible error types</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>awayTeam</td>
              <td>homeTeam</td>
              <td>10</td>
              <td>16</td>
              <td>0.625</td>
              <td>0.375</td>
              <td>Semantic label</td>
            </tr>
            <tr>
              <td>ratingCount</td>
              <td>reviewCount</td>
              <td>65</td>
              <td>199</td>
              <td>0.327</td>
              <td>0.568</td>
              <td>Semantic label</td>
            </tr>
            <tr>
              <td>height</td>
              <td>width</td>
              <td>57</td>
              <td>192</td>
              <td>0.297</td>
              <td>0.464</td>
              <td>Semantic label</td>
            </tr>
            <tr>
              <td>productID</td>
              <td>sku</td>
              <td>42</td>
              <td>154</td>
              <td>0.273</td>
              <td>0.26</td>
              <td>Semantic label, Duplicate label</td>
            </tr>
            <tr>
              <td>dateCreated</td>
              <td>datePublished</td>
              <td>62</td>
              <td>249</td>
              <td>0.249</td>
              <td>0.558</td>
              <td>Semantic label</td>
            </tr>
          </tbody>
        </table>
      </div>

      <span id="toc6"></span>
      <h2>6. Discussion and Conclusion</h2>
      This section is dedicated to the discussion of findings for the project.
      We then conclude the project and provide potential implications for future
      research in the field of table annotation.

      <span id="toc6.1"></span>
      <h3>6.1 Discussion</h3>
      <p>
        We draw inference on the discrepency of results on the SOTAB and
        WikiTable datasets, followed by our analysis on why Column-TaBERT
        serialization performed wose than its counterpart, TaBERT serialization.
      </p>
      <p><b>WikiTable dataset</b></p>
      <p>
        In <i>Table 3a</i> and <i>Table 3b</i>, we can see that the best
        serialization differs between the SOTAB and WikiTable datasets. One
        reason for this discrepancy could be that the WikiTable dataset contains
        more tables with fewer columns, such as tables having only 2 columns.
        The difference is significant as calculated in <i>Table 1</i>. This
        could cause the Neighboring Column method to perform less and result in
        a lower F1 score on the test set.
      </p>
      <p><b>Column-TaBERT</b></p>
      <p>
        We hypothesized that the Column-TaBERT method would be able to encode
        more information with same token limit as TaBERT, thus producing better
        results. However, the results indicate that the column data type
        performed worse than the cell data type. This may be due to the fact
        that the structural design did not effectively enable the attention
        mechanism. In TaBERT, the model has one data type token placed just
        before the cell value. In contrast, Column-TaBERT obtains the dat type
        only once and structures it as a normal cell value. Future experiments
        could explore ways to improve attention by altering this serialization
        design.
      </p>

      <span id="toc6.2"></span>
      <h3>6.2 Conclusion</h3>

      <p>
        Drawing a conclusion, we have achieved significant improvement on
        current methods for both CTA and CPA tasks for the SOTAB benchmark and
        comparatively on the WikiTables dataset. We conclude that for a
        Transformer-based language model, structureness in input sequence, both
        column serialization and row serialization plays significant role in
        attention mechanism. With a large training data like SOTAB benchmark, it
        is apparent that data augmentation methods, such as preprocessing and
        augmentation are less useful. However, relevant experiments should be
        conducted when dealing with smaller datasets.
      </p>

      <p>
        Moving forward, we were not able to fully utilize the potential of
        Longformer with more input sequence length. Additionally, due to time
        limit, we did not further develop specific data augmentation methods for
        different promising serialization methods. Lastly, optimization could be
        done to increase the training speed such as increasing batch size, model
        checkpointing and parallelism. On average, 1 epoch of training a RoBERTa
        model for SOTAB benchmark requires 1 hour for our machines and 5 times
        the duration for WikiTables dataset.
      </p>
      <p>
        As table annotation using deep learning is still at its infancy, future
        works may include Longformer, Contrastive Learning or Large Language
        Models like ChatGPT.
      </p>

      <span id="toc6.3"></span>
      <h3>6.3 Acknowledgments</h3>
      <p>
        The authors acknowledge support by the state of Baden-Wrttemberg
        through bwHPC.
      </p>

      <span id="toc7"></span>
      <h2>7. References</h2>

      <p>
        <span id="toc9"></span>
        [1] Korini, K., Peeters, R., & Bizer, C. (2022).
        <a
          href="https://www.semanticscholar.org/paper/SOTAB%3A-The-WDC-Schema.org-Table-Annotation-Korini-Peeters/4d76a2ed9115e5fb2abe3a2b43baf77bc3b4ac6d"
          >SOTAB: The WDC Schema.org Table Annotation Benchmark</a
        >. SemTab@ISWC.<br />

        <span id="toc10"></span>
        [2] Ritze, D., Lehmberg, O., Oulabi, Y., & Bizer, C. (2016).
        <a
          href="https://www.semanticscholar.org/paper/Profiling-the-Potential-of-Web-Tables-for-Knowledge-Ritze-Lehmberg/221fae16210697dd430e0c3c5be138704d0943f1"
        >
          Profiling the Potential of Web Tables for Augmenting Cross-domain
          Knowledge Bases</a
        >. Proceedings of the 25th International Conference on World Wide Web.
        <br />

        <span id="toc11"></span>
        [3] Rahm, E., & Bernstein, P.A. (2001).
        <a
          href="https://www.semanticscholar.org/paper/A-survey-of-approaches-to-automatic-schema-matching-Rahm-Bernstein/580221d63ae75bdc7d68829916cf608e44a56b27"
        >
          A survey of approaches to automatic schema matching</a
        >. The VLDB Journal, 10, 334-350. <br />

        <span id="toc12"></span>
        [4] Deng, X., Sun, H., Lees, A., Wu, Y., & Yu, C. (2020).
        <a
          href="https://www.semanticscholar.org/paper/TURL%3A-Table-Understanding-through-Representation-Deng-Sun/5b3d791caf682998bbd96ce08a98bfc95a86b3a6"
        >
          TURL: Table Understanding through Representation Learning</a
        >. ArXiv, abs/2006.14806. <br />

        <span id="toc13"></span>
        [5] Vaswani, A., Shazeer, N.M., Parmar, N., Uszkoreit, J., Jones, L.,
        Gomez, A.N., Kaiser, L., & Polosukhin, I. (2017).
        <a
          href="https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776"
        >
          Attention is All you Need</a
        >. ArXiv, abs/1706.03762. <br />

        <span id="toc14"></span>
        [6] Hulsebos, M., Demiralp, C., & Groth, P. (2021).
        <a
          href="https://www.semanticscholar.org/paper/GitTables%3A-A-Large-Scale-Corpus-of-Relational-Hulsebos-Demiralp/0a63aecfed066c3c69babba501738e88c0709a4c"
        >
          GitTables: A Large-Scale Corpus of Relational Tables</a
        >
        ArXiv, abs/2106.07258. <br />

        <span id="toc15"></span>
        [7] Suhara, Y., Li, J., Li, Y., Zhang, D., Demiralp, C., Chen, C., &
        Tan, W.C. (2021).
        <a
          href="https://www.semanticscholar.org/paper/Annotating-Columns-with-Pre-trained-Language-Models-Suhara-Li/1dbc9bc05805f799be4a5d20394767a01a8358de"
        >
          Annotating Columns with Pre-trained Language Models</a
        >. Proceedings of the 2022 International Conference on Management of
        Data. <br />

        <span id="toc16"></span>
        [8] Wang, Z., Dong, H., Jia, R., Li, J., Fu, Z., Han, S., & Zhang, D.
        (2020).
        <a
          href="https://www.semanticscholar.org/paper/TUTA%3A-Tree-based-Transformers-for-Generally-Table-Wang-Dong/24a12899ce97bd4a56f7c6b49d3979b9465f0190"
        >
          TUTA: Tree-based Transformers for Generally Structured Table
          Pre-training</a
        >. Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
        & Data Mining. <br />

        <span id="toc17"></span>
        [9] Chen, J., Jimnez-Ruiz, E., Horrocks, I., & Sutton, C. (2019).
        <a
          href="https://www.semanticscholar.org/paper/Learning-Semantic-Annotations-for-Tabular-Data-Chen-Jim%C3%A9nez-Ruiz/e33e897ca9646c57438378b6de935d44f6aaa4d6"
        >
          Learning Semantic Annotations for Tabular Data</a
        >. ArXiv, abs/1906.00781. <br />

        <span id="toc18"></span>
        [10] Khurana, U., & Galhotra, S. (2021).
        <a
          href="https://www.semanticscholar.org/paper/Semantic-Concept-Annotation-for-Tabular-Data-Khurana-Galhotra/f3012cc98e65e5fcc8acc782b4726452cd0bdd9d"
        >
          Semantic Concept Annotation for Tabular Data</a
        >. Proceedings of the 30th ACM International Conference on Information &
        Knowledge Management.<br />

        <span id="toc19"></span>
        [11] Zhang, D., Suhara, Y., Li, J., Hulsebos, M., Demiralp, C., & Tan,
        W.C. (2019).
        <a
          href="https://www.semanticscholar.org/paper/Sato%3A-Contextual-Semantic-Type-Detection-in-Tables-Zhang-Suhara/0983f358acd88e3559f4a763c90281d35768787e"
        >
          Sato: Contextual Semantic Type Detection in Tables</a
        >. Proc. VLDB Endow., 13, 1835-1848. <br />

        <span id="toc20"></span>
        [12] Brugman, S.
        <a href="https://github.com/pandas-profiling/">
          pandas-profiling: Exploratory Data Analysis for Python</a
        ><br />

        <span id="toc21"></span>
        [13] Beltagy, I., Peters, M.E., & Cohan, A. (2020).
        <a
          href="https://www.semanticscholar.org/paper/Longformer%3A-The-Long-Document-Transformer-Beltagy-Peters/71b6394ad5654f5cd0fba763768ba4e523f7bbca"
        >
          Longformer: The Long-Document Transformer</a
        >. ArXiv, abs/2004.05150.<br />

        <span id="toc22"></span>
        [14] Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019).
        <a
          href="https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992"
        >
          BERT: Pre-training of Deep Bidirectional Transformers for Language
          Understanding</a
        >. ArXiv, abs/1810.04805. <br />

        <span id="toc23"></span>
        [15] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,
        Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019).
        <a
          href="https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de"
        >
          RoBERTa: A Robustly Optimized BERT Pretraining Approach</a
        >. ArXiv, abs/1907.11692. <br />

        <span id="toc24"></span>
        [16] Nguyen, P., Kertkeidkachorn, N., Ichise, R., & Takeda, H. (2020).
        <a
          href="https://www.semanticscholar.org/paper/MTab4DBpedia%3A-Semantic-Annotation-for-Tabular-Data-Nguyen-Kertkeidkachorn/619241c8d29884b04269fa2b9251527eab7814e6"
        >
          MTab4DBpedia: Semantic Annotation for Tabular Data with DBpedia</a
        >.
        <br />

        <span id="toc25"></span>
        [17] Lin, T., Wang, Y., Liu, X., & Qiu, X. (2021).
        <a
          href="https://www.semanticscholar.org/paper/A-Survey-of-Transformers-Lin-Wang/d8d2e574965fe733eb1416e03df2b5c2914fc530"
        >
          A Survey of Transformers</a
        >. AI Open, 3, 111-132. <br />

        <span id="toc26"></span>
        [18] Bhagavatula, C., Noraset, T., & Downey, D. (2015).
        <a
          href="https://www.semanticscholar.org/paper/TabEL%3A-Entity-Linking-in-Web-Tables-Bhagavatula-Noraset/8ffcad9346c4978a211566fde6807d6fb4bfa5ed"
        >
          TabEL: Entity Linking in Web Tables</a
        >. International Workshop on the Semantic Web. <br />

        <span id="toc27"></span>
        [19] Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang,
        F., & Liu, Q. (2019).
        <a
          href="https://www.semanticscholar.org/paper/TinyBERT%3A-Distilling-BERT-for-Natural-Language-Jiao-Yin/0cbf97173391b0430140117027edcaf1a37968c7"
        >
          TinyBERT: Distilling BERT for Natural Language Understanding</a
        >. Findings. <br />

        <span id="toc28"></span>
        [20] Yin, P., Neubig, G., Yih, W., & Riedel, S. (2020).
        <a
          href="https://www.semanticscholar.org/paper/TaBERT%3A-Pretraining-for-Joint-Understanding-of-and-Yin-Neubig/a5b1d1cab073cb746a990b37d42dc7b67763f881"
        >
          TaBERT: Pretraining for Joint Understanding of Textual and Tabular
          Data</a
        >. ArXiv, abs/2005.08314. <br />

        <span id="toc29"></span>
        [21] Google (2015).
        <a href="https://developers.google.com/freebase/data">
          Freebase Data Dumps.
        </a>
        <br />
      </p>
      <p>&nbsp;</p>

      <script type="text/javascript">
        $("#toc").toc({
          selectors: "h2", //elements to use as headings
          container: "#toccontent", //element to find all selectors in
          smoothScrolling: true, //enable or disable smooth scrolling on click
          prefix: "toc", //prefix for anchor tags and class names
          highlightOnScroll: true, //add class to heading that is currently in focus
          highlightOffset: 100, //offset to trigger the next headline
          anchorName: function (i, heading, prefix) {
            //custom function for anchor name
            return prefix + i;
          },
        });
        $('[id*="link_"]').each(function () {
          var element = $(this);
          element.click(function (e) {
            e.preventDefault();
            var id = element.attr("id").split("_")[1];
            element.parent().removeClass("show").addClass("no-show");
            $("#charts_" + id)
              .removeClass("no-show")
              .addClass("show");
          });
        });
        $('[id*="link_"]').each(function () {
          var element = $(this);
          element.click(function (e) {
            e.preventDefault();
            var id = element.attr("id").split("_")[1];
            element.parent().removeClass("show").addClass("no-show");
            $("#charts_" + id)
              .removeClass("no-show")
              .addClass("show");
          });
        });
        $('[id*="colapse_"]').each(function () {
          var element = $(this);
          element.click(function (e) {
            e.preventDefault();
            var id = element.attr("id").split("_")[1];
            element.parent().removeClass("show").addClass("no-show");
            $("#intro_" + id)
              .removeClass("no-show")
              .addClass("show");
          });
        });
        document.getElementById("defaultOpen").click();
      </script>
    </div>
  </body>
</html>
