<!DOCTYPE html>
<html>
  <head>
    <title>Table Annotation using Deep Learning</title>
    <link
      rel="stylesheet"
      href="https://webdatacommons.org/style.css"
      type="text/css"
      media="screen"
    />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <style>
      .tar {
        text-align: right;
      }
      .rtable {
        float: right;
        padding-left: 10px;
      }
      .smalltable,
      .smalltable TD,
      .smalltable TH {
        font-size: 9pt;
      }
      .tab {
        overflow: hidden;
        border: 1px solid #ccc;
        background-color: #eaf3fa;
        clear: both;
        padding-left: 25px;
        width: 650px;
      }
      .tab button {
        background-color: inherit;
        float: left;
        border: none;
        outline: none;
        cursor: pointer;
        padding: 15px 60px;
        transition: 0.3s;
      }
      .tab button:hover {
        background-color: #ddd;
      }
      .tab button.active {
        background-color: #ccc;
      }
      .tabcontent {
        display: none;
        padding: 6px 12px;
        border-top: none;
        animation: fadeEffect 1s;
        width: 500px;
      }
      .table-wrapper {
        position: relative;
      }
      .table-scroll {
        height: 240px;
        overflow: auto;
        margin-top: -10px;
      }
      .show {
        display: block;
      }
      .no-show {
        display: none;
      }
      caption {
        caption-side: top;
        font-style: italic;
      }
      td[scope="mergedcol"] {
        text-align: center;
      }
      tr.bordered {
        border-bottom: 1px solid #000;
      }
      hr {
        width: 50%;
        margin: 20px 0;
        /* This leaves 10px margin on left and right. If only right margin is needed try margin-right: 10px; */
      }
      .tg {
        border-collapse: collapse;
        border-color: #ccc;
        border-spacing: 0;
      }
      .tg td {
        background-color: #fff;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg th {
        background-color: #f0f0f0;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        font-weight: normal;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg .tg-lboi {
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-9wq8 {
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-ixdq {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: center;
        top: -1px;
        vertical-align: middle;
        will-change: transform;
      }
      .tg .tg-mkpc {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: left;
        top: -1px;
        vertical-align: middle;
        will-change: transform;
      }
      .tg .tg-d459 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-kyy7 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-h2b0 {
        background-color: #fff;
        border-color: inherit;
        color: #333;
        text-align: center;
        vertical-align: middle;
      }
      .tg-sort-header::-moz-selection {
        background: 0 0;
      }
      .tg-sort-header::selection {
        background: 0 0;
      }
      .tg-sort-header {
        cursor: pointer;
      }
      .tg-sort-header:after {
        content: "";
        float: right;
        margin-top: 7px;
        border-width: 0 5px 5px;
        border-style: solid;
        border-color: #404040 transparent;
        visibility: hidden;
      }
      .tg-sort-header:hover:after {
        visibility: visible;
      }
      .tg-sort-asc:after,
      .tg-sort-asc:hover:after,
      .tg-sort-desc:after {
        visibility: visible;
        opacity: 0.4;
      }
      .tg-sort-desc:after {
        border-bottom: none;
        border-width: 5px 5px 0;
      }
      @media screen and (max-width: 767px) {
        .tg {
          width: auto !important;
        }
        .tg col {
          width: auto !important;
        }
        .tg-wrap {
          overflow-x: auto;
          -webkit-overflow-scrolling: touch;
        }
      }
      .tg {
        border-collapse: collapse;
        border-color: #ccc;
        border-spacing: 0;
      }
      .tg td {
        background-color: #fff;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg th {
        background-color: #f0f0f0;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        font-weight: normal;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg .tg-lboi {
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-9wq8 {
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-baqh {
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-zyik {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: center;
        top: -1px;
        vertical-align: top;
        will-change: transform;
      }
      .tg .tg-ixdq {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: center;
        top: -1px;
        vertical-align: middle;
        will-change: transform;
      }
      .tg .tg-yy5h {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-o939 {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-ufyq {
        background-color: #f0f0f0;
        font-weight: bold;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-asv9 {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-d459 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-dzk6 {
        background-color: #f9f9f9;
        text-align: center;
        vertical-align: top;
      }
      .tg .tg-kyy7 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg-sort-header::-moz-selection {
        background: 0 0;
      }
      .tg-sort-header::selection {
        background: 0 0;
      }
      .tg-sort-header {
        cursor: pointer;
      }
      .tg-sort-header:after {
        content: "";
        float: right;
        margin-top: 7px;
        border-width: 0 5px 5px;
        border-style: solid;
        border-color: #404040 transparent;
        visibility: hidden;
      }
      .tg-sort-header:hover:after {
        visibility: visible;
      }
      .tg-sort-asc:after,
      .tg-sort-asc:hover:after,
      .tg-sort-desc:after {
        visibility: visible;
        opacity: 0.4;
      }
      .tg-sort-desc:after {
        border-bottom: none;
        border-width: 5px 5px 0;
      }
      @media screen and (max-width: 767px) {
        .tg {
          width: auto !important;
        }
        .tg col {
          width: auto !important;
        }
        .tg-wrap {
          overflow-x: auto;
          -webkit-overflow-scrolling: touch;
        }
      }
      .tg {
        border-collapse: collapse;
        border-color: #ccc;
        border-spacing: 0;
        display: inline-block;
        margin-right: 50px;
      }
      .tg td {
        background-color: #fff;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg th {
        background-color: #f0f0f0;
        border-color: #ccc;
        border-style: solid;
        border-width: 1px;
        color: #333;
        font-family: Arial, sans-serif;
        font-size: 14px;
        font-weight: normal;
        overflow: hidden;
        padding: 10px 5px;
        word-break: normal;
      }
      .tg .tg-lboi {
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-pl4i {
        background-color: #f0f0f0;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-j1i3 {
        border-color: inherit;
        position: -webkit-sticky;
        position: sticky;
        text-align: left;
        top: -1px;
        vertical-align: top;
        will-change: transform;
      }
      .tg .tg-9wq8 {
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-ixdq {
        border-color: inherit;
        font-weight: bold;
        position: -webkit-sticky;
        position: sticky;
        text-align: center;
        top: -1px;
        vertical-align: middle;
        will-change: transform;
      }
      .tg .tg-yy5h {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-o939 {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-45e1 {
        background-color: #f0f0f0;
        border-color: inherit;
        font-weight: bold;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-nrix {
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-d459 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: left;
        vertical-align: middle;
      }
      .tg .tg-kyy7 {
        background-color: #f9f9f9;
        border-color: inherit;
        text-align: center;
        vertical-align: middle;
      }
      .tg .tg-57iy {
        background-color: #f9f9f9;
        text-align: center;
        vertical-align: middle;
      }
      .tg-sort-header::-moz-selection {
        background: 0 0;
      }
      .tg-sort-header::selection {
        background: 0 0;
      }
      .tg-sort-header {
        cursor: pointer;
      }
      .tg-sort-header:after {
        content: "";
        float: right;
        margin-top: 7px;
        border-width: 0 5px 5px;
        border-style: solid;
        border-color: #404040 transparent;
        visibility: hidden;
      }
      .tg-sort-header:hover:after {
        visibility: visible;
      }
      .tg-sort-asc:after,
      .tg-sort-asc:hover:after,
      .tg-sort-desc:after {
        visibility: visible;
        opacity: 0.4;
      }
      .tg-sort-desc:after {
        border-bottom: none;
        border-width: 5px 5px 0;
      }
      @media screen and (max-width: 767px) {
        .tg {
          width: auto !important;
        }
        .tg col {
          width: auto !important;
        }
        .tg-wrap {
          overflow-x: auto;
          -webkit-overflow-scrolling: touch;
        }
      }
      @keyframes fadeEffect {
        from {
          opacity: 0;
        }
        to {
          opacity: 1;
        }
      }
      .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 30%;
    }
    </style>
    <script
      type="text/javascript"
      src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"
    ></script>
    <script type="text/javascript" src="../../jquery.toc.min.js"></script>
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(["_setAccount", "UA-30248817-1"]);
      _gaq.push(["_trackPageview"]);

      (function () {
        var ga = document.createElement("script");
        ga.type = "text/javascript";
        ga.async = true;
        ga.src =
          ("https:" == document.location.protocol
            ? "https://ssl"
            : "http://www") + ".google-analytics.com/ga.js";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(ga, s);
      })();
    </script>
    <script type="application/ld+json">
      {
      	"@context": "http://schema.org/",
      	"@type": "Dataset",
      	"name": "Web Data Commons - Schema.org Table Corpus",
      	"description": "The product dataset consists of 20 million pairs of product offers referring to the same products. The offers were extracted from 43 thousand e-shops which provide schema.org annotations including some form of product ID such as a GTIN or MPN. We also created a gold standard by manually verifying 4400 pairs of offers belonging to four different product categories.",
      	"url": "http://webdatacommons.org/structureddata/schemaorgtables/index.html",
      	"keywords": [
      		"table corpus",
      		"product corpus",
      		"large scale",
      		"product matching",
      		"entity matching"
      	],
      	"creator": [
      		{
      			"@type": "Person",
      			"url": "https://www.uni-mannheim.de/dws/people/researchers/phd-students/ralph-peeters/"
      			"name": "Ralph Peeters"
      		},
      		{
      			"@type": "Person",
      			"url": "https://www.uni-mannheim.de/dws/people/professors/prof-dr-christian-bizer/",
      			"name": "Christian Bizer"
      		}
      	],
      	"distribution": [{
      		"@type": "DataDownload",
      		"fileFormat": [
      			"json"
      		],
      		//TODO: Change once final
      		"contentUrl": "http://webdatacommons.org/structureddata/sotab/index.html"
      	}],
      	"citation": [

      	]
      }
    </script>
    <script charset="utf-8">
      var TGSort =
        window.TGSort ||
        (function (n) {
          "use strict";
          function r(n) {
            return n ? n.length : 0;
          }
          function t(n, t, e, o = 0) {
            for (e = r(n); o < e; ++o) t(n[o], o);
          }
          function e(n) {
            return n.split("").reverse().join("");
          }
          function o(n) {
            var e = n[0];
            return (
              t(n, function (n) {
                for (; !n.startsWith(e); ) e = e.substring(0, r(e) - 1);
              }),
              r(e)
            );
          }
          function u(n, r, e = []) {
            return (
              t(n, function (n) {
                r(n) && e.push(n);
              }),
              e
            );
          }
          var a = parseFloat;
          function i(n, r) {
            return function (t) {
              var e = "";
              return (
                t.replace(n, function (n, t, o) {
                  return (e = t.replace(r, "") + "." + (o || "").substring(1));
                }),
                a(e)
              );
            };
          }
          var s = i(/^(?:\s*)([+-]?(?:\d+)(?:,\d{3})*)(\.\d*)?$/g, /,/g),
            c = i(/^(?:\s*)([+-]?(?:\d+)(?:\.\d{3})*)(,\d*)?$/g, /\./g);
          function f(n) {
            var t = a(n);
            return !isNaN(t) && r("" + t) + 1 >= r(n) ? t : NaN;
          }
          function d(n) {
            var e = [],
              o = n;
            return (
              t([f, s, c], function (u) {
                var a = [],
                  i = [];
                t(n, function (n, r) {
                  (r = u(n)), a.push(r), r || i.push(n);
                }),
                  r(i) < r(o) && ((o = i), (e = a));
              }),
              r(
                u(o, function (n) {
                  return n == o[0];
                })
              ) == r(o)
                ? e
                : []
            );
          }
          function v(n) {
            if ("TABLE" == n.nodeName) {
              for (
                var a = (function (r) {
                    var e,
                      o,
                      u = [],
                      a = [];
                    return (
                      (function n(r, e) {
                        e(r),
                          t(r.childNodes, function (r) {
                            n(r, e);
                          });
                      })(n, function (n) {
                        "TR" == (o = n.nodeName)
                          ? ((e = []), u.push(e), a.push(n))
                          : ("TD" != o && "TH" != o) || e.push(n);
                      }),
                      [u, a]
                    );
                  })(),
                  i = a[0],
                  s = a[1],
                  c = r(i),
                  f = c > 1 && r(i[0]) < r(i[1]) ? 1 : 0,
                  v = f + 1,
                  p = i[f],
                  h = r(p),
                  l = [],
                  g = [],
                  N = [],
                  m = v;
                m < c;
                ++m
              ) {
                for (var T = 0; T < h; ++T) {
                  r(g) < h && g.push([]);
                  var C = i[m][T],
                    L = C.textContent || C.innerText || "";
                  g[T].push(L.trim());
                }
                N.push(m - v);
              }
              t(p, function (n, t) {
                l[t] = 0;
                var a = n.classList;
                a.add("tg-sort-header"),
                  n.addEventListener("click", function () {
                    var n = l[t];
                    !(function () {
                      for (var n = 0; n < h; ++n) {
                        var r = p[n].classList;
                        r.remove("tg-sort-asc"),
                          r.remove("tg-sort-desc"),
                          (l[n] = 0);
                      }
                    })(),
                      (n = 1 == n ? -1 : +!n) &&
                        a.add(n > 0 ? "tg-sort-asc" : "tg-sort-desc"),
                      (l[t] = n);
                    var i,
                      f = g[t],
                      m = function (r, t) {
                        return n * f[r].localeCompare(f[t]) || n * (r - t);
                      },
                      T = (function (n) {
                        var t = d(n);
                        if (!r(t)) {
                          var u = o(n),
                            a = o(n.map(e));
                          t = d(
                            n.map(function (n) {
                              return n.substring(u, r(n) - a);
                            })
                          );
                        }
                        return t;
                      })(f);
                    (r(T) ||
                      r((T = r(u((i = f.map(Date.parse)), isNaN)) ? [] : i))) &&
                      (m = function (r, t) {
                        var e = T[r],
                          o = T[t],
                          u = isNaN(e),
                          a = isNaN(o);
                        return u && a
                          ? 0
                          : u
                          ? -n
                          : a
                          ? n
                          : e > o
                          ? n
                          : e < o
                          ? -n
                          : n * (r - t);
                      });
                    var C,
                      L = N.slice();
                    L.sort(m);
                    for (var E = v; E < c; ++E)
                      (C = s[E].parentNode).removeChild(s[E]);
                    for (E = v; E < c; ++E) C.appendChild(s[v + L[E - v]]);
                  });
              });
            }
          }
          n.addEventListener("DOMContentLoaded", function () {
            for (var t = n.getElementsByClassName("tg"), e = 0; e < r(t); ++e)
              try {
                v(t[e]);
              } catch (n) {}
          });
        })(document);
    </script>
    <!-- MathJax: cdn to display latex equations -->
  <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  </head>
  <body>
    <div id="logo" style="text-align: right; background-color: white">
      &nbsp;&nbsp;<a href="http://dws.informatik.uni-mannheim.de"
        ><img src="images/ma-logo.gif" alt="University of Mannheim - Logo"
      /></a>
    </div>
    <div id="header">
      <h1 style="font-size: 250%">Table Annotation using Deep Learning</h1>
    </div>
    <div id="authors">
      <a>Christian Bizer</a><br />
      <a>Chun-Yi Chen</a><br />
      <a>I-Chen Hsieh</a><br />
      <a>Keti Korini</a><br />
      <a>Munir Abobaker</a><br />
      <a>Reng Chiz Der</a><br />
    </div>

    <div id="content">
      <p>
        This webpage engages with the experiements and results of a six month
        student team project on the topic of "Table Annotation using Deep
        Learning" at the School of Business Informatics and Mathematics of the
        University of Mannheim under the supervision of Keti Korini and
        Christian Bizer.
      </p>
      <p>
        We have conducted a set of experiments around the use of transformer
        language models based on
        <a href="https://webdatacommons.org/structureddata/sotab/"
          >WDC Schema.org Table Annotation Benchmark (SOTAB)</a
        >[<a href="#toc9">1</a>]. There are two multi-class classification
        tasks:
        <a href="https://paperswithcode.com/task/column-type-annotation"
          >Column Type Annotation (CTA)</a
        >
        and
        <a href="https://paperswithcode.com/task/columns-property-annotation"
          >Columns Property Annotation (CPA)</a
        >. CTA involves predicting the column type, while CPA involves
        predicting the column properties or relations. All of our experiments
        and code are available in our
        <a
          href="https://github.com/wbsg-uni-mannheim-students/table-annotation-using-deep-learning"
          >GitHub repository.</a
        >
      </p>

      <p>
        The organization of this work is as follows. Firstly, we will introduce and motivate the task of table 
        annotation in Chapter 1. Next, in Chapter 2, we will provide an overview of the preliminaries,such as CTA, CPA, and <br> Transformers. Chapter 3 will then present the related work.
        Moving on to Chapters 4 and 5, we will delve into the experimental results and Error Analysis. Finally, in Chapters 6 and 7, 
        we will discuss the results <br>and conclude the work, respectively.
      </p>
      <h2>Contents</h2>
      <ul>
        <li class="toc-h2 toc-active">
          <a href="#toc1"> 1. Introduction</a>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc2"> 2. Theoretical Background</a>
          <ul>
            <li>
              <a href="#toc2.1">2.1 CTA</a>
            </li>
            <li>
              <a href="#toc2.2">2.2 CPA</a>
            </li>
            <li>
              <a href="#toc2.3">2.3 Transformer</a>
            </li>
          </ul>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc3"> 3. Related Work</a>
          <ul>
            <li>
              <a href="#toc3.1">3.1 Transformers</a>
            </li>
          </ul>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc4"> 4. Model </a>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc5"> 5. Evaluation </a>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc6"> 6. Experimental Results and Error Analysis </a>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc7"> 7. Conclusion </a>
        </li>
        <li class="toc-h2 toc-active">
          <a href="#toc8"> 8. References </a>
        </li>
      </ul>
      <span id="toc1"></span>
      <h2>1. Introduction</h2>
      <p>
        Table annotation is the process of adding descriptive labels to tables
        in a large dataset using concepts from a knowledge graph or shared
        vocabulary. This process is crucial for effective data search and
        integration, especially since the web contains millions of high-quality
        tables available on websites and public data portals such as Wikipedia.
        However, to utilize these tables, we need to understand their structure
        and schema, which can be challenging due to their heterogeneous or
        unknown headers and content. The objective of this project is to address
        the aforementioned challenges by proficiently utilizing diverse deep
        learning methods for both Column Type annotation and Columns Property
        Annotation tasks.
      </p>

      <span id="toc2"></span>
      <h2>2. Theoretical Background</h2>
      <p>
        This Chapter provides an overview of the theoretical underpinnings,
        frameworks as well as specific information relevant to follow our work.
        For this reason, we start by introducing the main tasks we are trying to
        solve. We also give a brief introduction to transformer models which
        formed the basis for the algorithms we use.
      </p>
      <span id="toc2.1"></span>

      <h3>2.1 Column Type Annotation (CTA)</h3>
      <p>
        Column Type Annotation (CTA) is a critical task in data integration and knowledge discovery that involves 
        assigning labels to each column in a table based on the types of entities it contains such as <b>"hotel/name",
        "streetAddress","addressLocality", "Country", and "currency" </b> as shown in <a href="#toc2.1.Fig">Figure 1</a>. Additionally, 
        CTA can also assign data types to columns such as String, Integer, and Boolean. While data types provide 
        basic information about the type of data stored in a column, entity types capture domain-specific semantics that 
        convey more meaning.  For instance, labeling a column with <b>"hotel/name"</b> rather than 
        just <b>"String" </b> provides valuable information about the specific type of data stored in that column, which is crucial for 
        downstream tasks such as data analysis and retrieval. Therefore, it can be argued that entity types 
        assigned through CTA are more useful than data types alone. [<a href="#toc8">1</a>,  <a href="#toc14">4</a>]
      
      </p>
      <span id="toc2.1.Fig"></span>
      <figure>
        <img src="images/table_CTA_CPA.jpg" alt="Table Annotation Example" class="center">
        <figcaption class="center"><b><a id="Fig1"></a>Figure 1:</b> Example of table annotation. The CTA labels are placed on the top of
          the columns, while the CPA labels are placed between the main column (leftmost column) and another column [<a href="#toc8">1</a>].
        </figcaption>
      </figure>
    
      <p>
          Examples of CTA are: 
        <ul>
          <li>
            In a customer database that includes columns for the customer name, transaction date, and purchase 
            amount. CTA can be used to identify the data type of each column, such as string for customer name, date 
            for transaction date, and float for purchase amount (data type). 
          </li>
          <li> 
            In a medical database,  that includes columns for patient ID, 
            diagnosis, and treatment. CTA can be used to identify the data type of each column, such as integer for 
            patient ID, string for diagnosis, and string for treatment (data type).   
          </li>
          <li>
            A financial institution has a database of stock market data that includes columns for stock symbol, date,
            and closing price. CTA can be used to identify the data type of each column, such as string for stock symbol,
            date for date, and float for closing price (data type). 
          </li>
          <li>
            An e-commerce website may have a table of products with columns such as "product name", "description", 
            "price", "category", and "brand". By annotating the columns with entity types such as "product name", 
            "brand", and "category", a machine learning model can better understand the semantic meaning of the data,
             and accurately categorize products into appropriate categories, recommend related products, and provide
              a more personalized shopping experience to the user (entity type).
          </li>
    
        </ul>
      </p>
      <p>
        Identifying the data type of each column in a table is essential because it can determine the preprocessing 
        steps required for that column. For instance, a column of type string may require spelling correction and 
        lemmatization, while a column of type currency may require unifying the format of currency values that 
        comes form different countries.
      </p>
    
        <p> 
          CTA can be approached as either <a href="https://paperswithcode.com/task/column-type-annotation/"> 
            a multi-class or multi-label classification</a> problem. In multi-class, each column is annotated with
          only one label representing its type, while in multi-label, each column is annotated with multiple labels.
       </p>

      <span id="toc2.2"></span>
      <h3>2.2 Columns Property Annotation (CPA)</h3>
      <p>
        CPA, a web data integration task, aims to identify relationships between different columns in a table. 
        It can infer relationships that might not be immediately obvious. For example,
         when applied to a table with columns labeled "hotel/name" and "address", CPA can help infer that the 
         address column refers to the hotel's location (see <a href="#toc2.1.Fig">Figure 1</a>). 
      </p>
  
        Here are a few examples of how CPA can be applied:<br>
      <ul>
        <li>In a product database that contains information about products, including their names, prices, and
         descriptions. CPA can be used to identify which columns are related to each other, such as the relationship 
         between the product name and description. </li>
   
         <li>In a customer database, CPA can be used to identify relationships between different columns such as the 
        relationship between a customer's name, age, and gender. </li>
    
        <li>In a medical database, CPA can be used to identify relationships between different columns such as the 
        relationship between a patient's symptoms, diagnosis, and treatment. </li>
      </ul>
        In each of these examples, CPA can help to identify relationships between columns that may not be 
        immediately apparent, leading to better data discovery. For example, in a product database, identifying the
        relationship between a product's name and description can help with search engine 
        optimization and online sales. Moreover, CPA is a fundamental technique in further 
        data management tasks. It plays a crucial role in ensuring  data quality control, as well as schema
         matching. [<a href="#toc17">7</a>].
      <p>
          One common way to tackle CPA is to view it as a <a href="https://paperswithcode.com/task/columns-property-annotation/"> multi-class classification</a> task, and it is also known as 
          column relation annotation or relation extraction in various works.
      </p>
      </p>

      <span id="toc2.3"></span>
      <h3>2.3 Transformer</h3>
      <p>

        Transformers are a type of neural network architecture that has become widely used in natural language 
        processing and other machine learning applications. They were first introduced in 2017 by Vaswani et al [<a href="#toc15">5</a>]. and
        have since become one of the most popular deep-learning models. Transformers are based on the concept of
        self-attention, which allows the model to focus on different parts of the input sequence at each step. 
      </p>
      <p>
  
        <p>
          The key innovation of transformers is self-attention, which is a mechanism that allows the 
          model to selectively focus on different parts of the input sequence. Self-attention works by computing a 
          weighted sum of the input sequence at each position, where the weights are determined by a similarity 
          function between the current position and all other positions. The output of the self-attention layer is a 
          sequence of weighted sums, which can then be fed into subsequent layers of the network.
          The attention function can be formalized as follows:
          <p><span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
          <p>where Q, K, and V are the query, key, and value vectors, respectively.
        </p>
        The attention mechanism can be used in the CTA task to help the model identify which parts of the input table
        are most relevant for predicting the column types. For example, the model might attend to the cells in the 
        first row of the table to identify common patterns or formats, such as whether the cells contain numerical 
        values or text.
      </p>
      <span id="toc2.3.fig"></span>
      <figure>
        <img src="images/transformers-architecture.JPG" alt="Transformer", class="center">
        <figcaption class="center"><b><a id="Fig1"></a >Figure 2:</b> The Transformer - model architecture [<a href="#toc15">5</a>].
        </figcaption>
      </figure>
  
      <p>
        As illustrated in <a href="#toc2.3.Fig">Figure 2</a>, the transformer architecture consists of an encoder and a decoder, which are both composed of multiple 
        layers of self-attention(Multi-head Attention) and feed-forward networks. The encoder takes an input sequence and produces a 
        sequence of encoded representations, while the decoder takes a sequence of encoded representations and 
        produces a sequence of outputs.
        One of the advantages of transformers is that they can process the entire input sequence at once, rather 
        than sequentially like traditional recurrent neural networks. This makes them more efficient and allows 
        them to capture longer-range dependencies[<a href="#toc15">5</a>].
      </p>
      <p>
        Example Application: Language Translation One of the most common applications of transformers is language
        translation. The core idea in a nutshell is, the input sequence is a sentence in one language, and the output sequence 
         is a sentence in another language. The encoder takes the input sentence and uses the self-attention to produce
        a sequence of encoded representations, which are then fed into the decoder. The decoder uses self-attention to 
        focus on different parts of the encoded sequence at each step, and produces a sequence of output representations. Finally, 
        a Softmax layer is  used to the produce probability distribution over the target language vocabulary.
        
      </p>

      <span id="toc3"></span>
      <h2>3. Related Work</h2>
      <p>
        <!--We need to discuss some existing papers that we have referred here. I.e.
        the approach by DODUO, TURL and maybe TUTA. -->
        With the emergence of pretraining/fine-tuning techniques, many researchers have utilized these methods in table annotation tasks. One such approach is presented in the paper
        [<a href="#toc12">4</a>],  which proposes a framework for table understanding through representation learning called TURL. The authors utilize the pre-training/fine-tuning paradigm to learn
         contextualized representations on relational tables during pretraining. This enables the model to be fine-tuned on various downstream tasks with minimal task-specific adjustments.
          The proposed model is pretrained in an unsupervised manner on a large-scale web table corpus that is based on the WikiTable corpus. Furthermore, the framework can also be used to 
          infer meta information about tables such as column types and relationships between columns (CTA and CPA).
         DUDO, another framework proposed by [<a href="#toc15">7</a>],  tackles the problems of annotating table columns using only the information contained in tables. It is built 
        around language models, and takes the entire table as input to predict column types and relationships between columns. This approach is unique in that it requires no external
        knowledge or context beyond what is present in the table itself. Furthermore, DUDO has achieved competitive results on two benchmarks  for the column type prediction and 
        column relation prediction tasks.
        [<a href="#toc16">8</a>] introduces TUTA, a pre-training architecture that enhances transformers for understanding generally structured tables. The authors use a structure aware mechanism that 
        employs a tree-based attention and position mechanism to capture the spatial and hierarchical information of tables. TUTA is trained on unlabeled web and spreadsheet tables at three 
        representation levels - token, cell, and table levels. It achieves state-of-the-art performance on five datasets for cell type classification and table type classification tasks.
      </p>


      <span id="toc4"></span>
      <h2>4. Model</h2>
      <p>
        A Transformer-based language model fits perfectly in the tasks pertaining table understanding for its language understanding capability and Transformer's attention mechanism that captures contextualized column representations. Such language models expect token sequences as inputs. This translates to representing relational tables in text sequences, formally known as table serialization, for the tasks of CTA and CPA. A natural serialization solution is single-column serialization that simply concatenate the column values into a sequence to feed into the language model. More concretely, as defined by Suhara et al.[<a href="#toc15">7</a>], for column <i>C</i> with column values <i>v<sub>1</sub>,...,v<sub>m</sub></i>, the serialized sequence is [<a href="#toc15">7</a>]: <p><span class="math display">\[serialize_{single}(C)::= [CLS]\;v_1\;...\;v_m\;[SEP]\]</span></p>
      </p>
      <p>
        Such serialization conveniently translates the CTA and CPA tasks into sequence classification and sequence-pair classification task respectively. The major drawback of single-column serialization is treating each column in a table as independent sequence neglects the table context that is crucial for relational tables that defines an real-world entity as a table. Previous works [<a href="#toc15">7, </a><a href="#toc17">9, </a><a href="#toc18">10, </a><a href="#toc19">11</a>] have highlighted the importance of table context for the CTA task.
      </p>

      <span id="toc4.1"></span>
      <h3>4.1 Preprocessing and Augmentation</h3>
      <p>
        To accommodate the token limit of 512 tokens for BERT-like language models, we trim cell values for longer textual data to fit more cells from a table. Following Wang et al.[<a href="#toc16">8</a>]'s empirical studies, we retain at most 8 tokens from a cell. This is because long textual strings often introduce noise and disrupt the structure of a table input. Additionally, we experiment with different threshold cell lengths, i.e., <em>median</em> and <em>mean</em> cell length computed per table basis. The rational being a table typically contains columns with mainly short string and numeric (date) values, and only a few long textual columns for descriptions.
      </p>
      <p>
        To enhance our data processing, we perform data augmentation (DA) at both the cell and table level during the earlier stages of our workflow. Specifically, we apply standard textual DA techniques, such as word swapping, word replacement, word deletion, and column cell value shuffling, at the cell level. At the table level, we perform random cell deletion and column cell shuffling.
      </p>

      <span id="toc4.2"></span>
      <h3>4.3 Serialization</h3>
      <p>
        <b>(i) Neighboring column</b>
      </p>
      <img
        src="./images/Neighbor.gif"
        alt="TaBERT illustration GIF"
        style="width: 550px; height: 330px"
      />
      <p>
        <b>(ii) TaBERT</b> The TaBERT serialization
        mainly focuses on the relationship between the target column and the
        context columns (all other columns) by inputting the context data in a
        row-based direction. Additionally, we were inspired by the architecture
        of TUTA[<a href="#toc16">8</a>], so we prepend a datatype token to each
        cell in this method. We expect this approach to allow the model to learn
        the relationships between the different columns and the target column,
        thereby enhancing the connection between each cell's value and the
        prediction target. For instance, the URL datatype may be present in the
        logo (which may imply the company or brand), hotel photos, etc.
      </p>
      <img
        src="./images/TaBERT.gif"
        alt="TaBERT illustration GIF"
        style="width: 550px; height: 330px"
      />
      <p>
        <b>(iii) Others</b>
      </p>
      <span id="toc4.4"></span>
      <h3>4.4 Further Model</h3>
      <p>
        <b>(i) Two-Step Model:</b>
      </p>
      <p>
        <b>(ii) Subtable Model:</b>
      </p>
      <p>
        <b>(ii) LongFormer:</b>
      </p>

      <span id="toc5"></span>
      <h2>5. Experiment</h2>

      <span id="toc5.1"></span>
      <h3>5.1 Dataset</h3>
      <p>
        We evaluate our solutions primarily on the
        <a href="https://webdatacommons.org/structureddata/sotab/"
          >WDC Schema.org Table Annotation Benchmark (SOTAB)</a
        >[<a href="#toc9">1</a>] benchmark dataset. To ensure the
        generalizability of our approach, we perform transfer learning to the
        <a href="https://github.com/sunlab-osu/TURL/"
          >WikiTables dataset by TURL</a
        >[<a href="#toc12">3</a>]. The SOTAB benchmark is a collection of tables
        extracted from the Schema.org Table Corpus, which is maintained by the
        Data and Web Science Research Group at the University of Mannheim. The
        corpus comprises over 4.2 million web relational tables covering 43
        schema.org classes. The dataset defines both CTA and CPA labels for
        tables from 17 schema.org classes/domains. For the CTA task, the SOTAB
        dataset provides annotations for 162,351 columns from 59,548 tables,
        with a label space of 91 type labels. For the CPA task, it provides
        annotations for 174,998 column pairs from 48,379 tables, with a label
        space of 176 property labels. We used the provided train/valid/test
        splits for both tasks.
      </p>
      <p>
        The WikiTables dataset [<a href="#toc12">3</a>] is a collection of
        tables extracted from the WikiTable corpus, which contains over 1.54
        million tables extracted from Wikipedia pages. The dataset defines both
        CTA and CPA labels. For the CTA task, the dataset provides annotations
        of 628,254 columns from 397,098 tables, with a label space of 255 type
        labels for training, and 13,025 (13,391) columns from 4,764 (4,844)
        tables for test and validation, respectively. For the CPA task, it
        provides annotations of 62,954 column pairs from 52,943 tables, with a
        label space of 121 relation labels for traning, and 2,072 (2,175) column
        pairs from 1,467 (1,560) tables for test (validation).
      </p>

      <span id="toc5.2"></span>
      <h3>5.2 Baselines</h3>
      <p>
        Our baseline configuration adopts the single column method, while all
        other parameters remain unchanged from the default values. Additionally,
        we take the TURL and DODUO result of SOTAB dataset from
        <a href="https://webdatacommons.org/structureddata/sotab/"
          >WDC Schema.org Table Annotation Benchmark (SOTAB)</a
        >[<a href="#toc9">1</a>] and result of WikiTable from the studies by
        Wang et al. [<a href="#toc16">8</a>]
      </p>

      <span id="toc5.3"></span>
      <h3>Experimental Settings</h3>
      <p>
        The default parameter settings are as follows:
      </p>
      </p>
      <ul>
        <li>Base Model: RoBERTa</li>
        <li>Learning Rate: 5e-5</li>
        <li>Max Length: 512</li>
        <li>Batch Size: 32</li>
        <li>Random Seeds: 0-2 (Take average as result F1)</li>
        <li>Window Size(if needed): 5</li>
      </ul>

      <span id="toc6"></span>
      <h2>6. Experimental Results and Error Analysis</h2>

      <span id="toc6.1"></span>
      <h3>6.1 Results and Ablation Study</h3>
      <p></p>
      <div class="tg-wrap">
        <table class="tg">
          <caption>
            Table ?: Experiment results
          </caption>
          <thead>
            <tr>
              <th class="tg-ixdq">Datset</th>
              <th class="tg-ixdq" colspan="2">SOTAB</th>
              <th class="tg-ixdq" colspan="2">WikiTable</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th class="tg-ixdq"><br /></th>
              <th class="tg-ixdq">CTA</th>
              <th class="tg-ixdq">CPA</th>
              <th class="tg-ixdq">CTA</th>
              <th class="tg-ixdq">CPA</th>
            </tr>
            <tr>
              <td class="tg-9wq8">Single Column</td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
            </tr>
            <tr>
              <td class="tg-9wq8">Neighboring column</td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
            </tr>
            <tr>
              <td class="tg-9wq8">TaBERT</td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
            </tr>
            <tr>
              <td class="tg-9wq8">DODUO</td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
            </tr>
            <tr>
              <td class="tg-9wq8">TURL</td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
              <td class="tg-9wq8"></td>
            </tr>
          </tbody>
        </table>
      </div>


      <span id="toc6.2"></span>
      <h3>6.2 Error Analysis</h3>
      <p>
        We performed error analysis on the best result obtained from neighbor
        serialization with a window size of 2. To do this, we sampled errors in
        prediction by schema for both the CTA and CPA tasks. Specifically, we
        sampled 614 errors from a total of 1533 observations for CTA, and 617
        errors from 3853 observations for CPA. We manually checked these tables
        and classified the types of errors we found, which are listed in
        <i>Table 1</i>.
      </p>
      <table>
        <caption>
          Table 1: Error Types and Definitions
        </caption>
        <thead>
          <th>Observed Error Types</th>
          <th>Definition</th>
          <th>Example</th>
        </thead>
        <tbody>
          <tr>
            <td>Actual wrong label</td>
            <td>The label given in the table is obviously incorrect.</td>
            <td>10-digit-number is labeled as gtin8</td>
          </tr>
          <tr>
            <td>Bad prediction</td>
            <td>
              The data provided by the column and neighbor columns is sufficient
              but the prediction is incorrect.
            </td>
            <td>gtin12 is predicted as gtin14</td>
          </tr>
          <tr>
            <td>Schema related label</td>
            <td>
              The prediction correctly represents the data but does not match
              the associated schema.
            </td>
            <td>In Movie schema, Movie/name is predicted as Book/name</td>
          </tr>
          <tr>
            <td>Semantic label</td>
            <td>
              The prediction has the same data type or format as label, but the
              interpretation of the data was incorrect.
            </td>
            <td>FAXnumber is predicted as telephone number and vice versa</td>
          </tr>
          <tr>
            <td>Duplicate label</td>
            <td>
              Two columns in a table that have same values but are labeled
              differently, leading to data redundancy and potential confusion or
              inconsistency in the interpretation of the data. In this case,
              usually the wrong prediction is the label of the other column.
            </td>
            <td>
              columns with same value are labeled as Brand and Organization
              respectively
            </td>
          </tr>
          <tr>
            <td>Hierarchical label</td>
            <td>
              The prediction may represent a higher or lower level of
              abstraction than the label, but the semantic meaning is still
              understandable.
            </td>
            <td>
              <ul>
                <li>Book/name is predicted as CreativeWork</li>
                <li>Restaurant is predicted as LocalBusiness</li>
              </ul>
            </td>
          </tr>
          <tr>
            <td>Bad tables</td>
            <td>
              Approximately 80% of the values in the table are either empty
              strings, 'undefined', or contain erroneous data inputted by
              humans.
            </td>
            <td></td>
          </tr>
          <tr>
            <td>Lack of information</td>
            <td>
              The data provided is insufficient in current and neighbor columns
              but contained in columns outside the windows.
            </td>
            <td></td>
          </tr>
        </tbody>
      </table>

      <span id="toc6"></span>
      <h2>6. Discussion</h2>
      <p></p>

      <span id="toc7"></span>
      <h2>7. Conclusion</h2>

      <p></p>

      <span id="toc8"></span>
      <h2>8. References</h2>

      <p>
        <span id="toc9"></span>
        [1] K. Korini, R. Peeters, C. Bizer, SOTAB: The WDC Schema.org Table
        Annotation Benchmark, in: Semantic Web Challenge on Tabular Data to
        Knowledge Graph Matching (SemTab), CEUR-WS.org, 2022. <br />

        <span id="toc10"></span>
        [2] D. Ritze, O. Lehmberg, Y. Oulabi, C. Bizer, Profiling the Potential
        of Web Tables for Augmenting Cross-domain Knowledge Bases, in:
        Proceedings of the 25th International Conference on World Wide Web,
        2016, pp. 251–261.
        <br />

        <span id="toc11"></span>
        [3] E. Rahm, P. A. Bernstein, A survey of approaches to automatic schema
        matching, The VLDB Journal — The International Journal on Very Large
        Data Bases 10 (2001) 334–350. <br />

        <span id="toc12"></span>
        [4] X. Deng, H. Sun, A. Lees, Y. Wu, C. Yu, TURL: Table understanding
        through representation learning, Proceedings of the VLDB Endowment 14
        (2020) 307–319. <br />

        <span id="toc13"></span>
        [5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, et al.,
        Attention is All you Need, in: Advances in Neural Information Processing
        Systems (NIPS 2017), volume 30, 2017. <br />

        <span id="toc14"></span>
        [6] M. Hulsebos, Ç. Demiralp, P. Groth, GitTables: A Large-Scale Corpus
        of Relational Tables, arXiv:2106.07258 (2022). <br />

        <span id="toc15"></span>
        [7] Suhara, Y., Li, J., Li, Y., Zhang, D., Demiralp, Ç. et al.
        “Annotating columns with pre-trained language models.” In Proceedings of
        the 2022 International Conference on Management of Data, 2022, pp.
        1493–1503 <br />

        <span id="toc16"></span>
        [8] Wang, Z., Dong, H., Jia, R., Li, J., Fu, Z., Han, S., & Zhang, D.
        (2020). TUTA: Tree-based Transformers for Generally Structured Table
        Pre-training. Proceedings of the 27th ACM SIGKDD Conference on Knowledge
        Discovery & Data Mining.

        <span id="toc17"></span>
        [9] Chen, J., Jiménez-Ruiz, E., Horrocks, I., & Sutton, C. (2019). Learning Semantic Annotations for Tabular Data. ArXiv, abs/1906.00781.

        <span id="toc18"></span>
        [10] Khurana, U., & Galhotra, S. (2021). Semantic Concept Annotation for Tabular Data. Proceedings of the 30th ACM International Conference on Information & Knowledge Management.

        <span id="toc19"></span>
        [11] Zhang, D., Suhara, Y., Li, J., Hulsebos, M., Demiralp, C., & Tan, W.C. (2019). Sato: Contextual Semantic Type Detection in Tables. Proc. VLDB Endow., 13, 1835-1848.
      </p>

      <p>&nbsp;</p>

      <script type="text/javascript">
        $("#toc").toc({
          selectors: "h2", //elements to use as headings
          container: "#toccontent", //element to find all selectors in
          smoothScrolling: true, //enable or disable smooth scrolling on click
          prefix: "toc", //prefix for anchor tags and class names
          highlightOnScroll: true, //add class to heading that is currently in focus
          highlightOffset: 100, //offset to trigger the next headline
          anchorName: function (i, heading, prefix) {
            //custom function for anchor name
            return prefix + i;
          },
        });
        $('[id*="link_"]').each(function () {
          var element = $(this);
          element.click(function (e) {
            e.preventDefault();
            var id = element.attr("id").split("_")[1];
            element.parent().removeClass("show").addClass("no-show");
            $("#charts_" + id)
              .removeClass("no-show")
              .addClass("show");
          });
        });
        $('[id*="link_"]').each(function () {
          var element = $(this);
          element.click(function (e) {
            e.preventDefault();
            var id = element.attr("id").split("_")[1];
            element.parent().removeClass("show").addClass("no-show");
            $("#charts_" + id)
              .removeClass("no-show")
              .addClass("show");
          });
        });
        $('[id*="colapse_"]').each(function () {
          var element = $(this);
          element.click(function (e) {
            e.preventDefault();
            var id = element.attr("id").split("_")[1];
            element.parent().removeClass("show").addClass("no-show");
            $("#intro_" + id)
              .removeClass("no-show")
              .addClass("show");
          });
        });
        document.getElementById("defaultOpen").click();
      </script>
    </div>
  </body>
</html>
